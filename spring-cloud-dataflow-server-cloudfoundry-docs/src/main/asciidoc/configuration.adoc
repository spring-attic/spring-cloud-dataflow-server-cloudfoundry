[[configuration]]
= Server Configuration

This section describes how to configure Spring Cloud Data Flow server's features, such as security and which relational database to use.
It also describes how to configure Spring Cloud Data Flow shell's features.

[[enable-disable-specific-features]]
== Feature Toggles

Data Flow server offers a specific set of features that you can enable or disable when launching. These features include all the lifecycle operations and REST endpoints (server, client implementations including Shell and the UI) for:

* Streams
* Tasks
* Analytics

You can enable or disable these features by setting the following boolean properties when you launch the Data Flow server:

* `spring.cloud.dataflow.features.streams-enabled`
* `spring.cloud.dataflow.features.tasks-enabled`
* `spring.cloud.dataflow.features.analytics-enabled`

By default, all features are enabled.
Note: Since the analytics feature is enabled by default, the Data Flow server is expected to have a valid Redis store available as its analytic repository (we provide a default implementation of analytics based on Redis). This also means that the Data Flow server's `health` depends on the redis store availability as well. If you do not want to enable HTTP endpoints to read analytics data written to Redis, you can disable the analytics feature by using the `spring.cloud.dataflow.features.analytics-enabled` property to `false`.

The REST endpoint (`/features`) provides information on the enabled and disabled features.

[[configuration-app-defaults]]
== Deployer Properties
You can also set other optional properties that alter the way Spring Cloud Data Flow deploys stream and task apps to Cloud Foundry:

* You can configure the default memory and disk sizes for a deployed application. By default, they are 1024 MB memory
and 1024 MB disk. To change these to (for example) 512 and 2048 respectively, use the following commands:
+
====
[source]\
----
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_MEMORY 512
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_DISK 2048
----
====

* The default number of instances to deploy is set to 1, but you can override it by using the following command:
+
====
[source]
----
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_INSTANCES 1
----
====

* You can set the buildpack that is used to deploy each application. For example, to use the Java offline buildback,
set the following environment variable:
+
====
[source]
----
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_BUILDPACK java_buildpack_offline
----
====

* You can customize the health check mechanism used by Cloud Foundry to assert whether apps are running by using the `SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_HEALTH_CHECK` environment variable. The current supported options
are `http` (the default), `port`, and `none`.
+
You can also set environment variables that specify the the HTTP-based health check endpoint and timeout: `SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_HEALTH_CHECK_ENDPOINT` and `SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_HEALTH_CHECK_TIMEOUT`, respectively. These default to `/health` (the Spring Boot default location) and `120` seconds.

* You can also specify deployment properties by using the DSL. For instance, if you want to set the allocated memory for the `http` application to 512m and also bind a mysql service to the `jdbc` application, you can run the following commands:
+
====
[source]
----
dataflow:> stream create --name mysqlstream --definition "http | jdbc --tableName=names --columns=name"
dataflow:> stream deploy --name mysqlstream --properties "deployer.http.memory=512, deployer.jdbc.cloudfoundry.services=mysql"
----
====
+
[NOTE]
=====
You can configure these settings separately for stream and task apps. To alter settings for tasks,
substitute `TASK` for `STREAM` in the property name, as the following example shows:

====
[source]
----
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_MEMORY 512
----
====
=====

TIP: All the properties mentioned above are `@ConfigurationProperties` of the
Cloud Foundry deployer. See link:https://github.com/spring-cloud/spring-cloud-deployer-cloudfoundry/blob/{deployer-branch-or-tag}/src/main/java/org/springframework/cloud/deployer/spi/cloudfoundry/CloudFoundryDeploymentProperties.java[CloudFoundryDeploymentProperties.java] for more information.

[[configuration-app-names-cloud-foundry]]
== Application Names and Prefixes

To help avoid clashes with routes across spaces in Cloud Foundry, a naming strategy that provides a random prefix to a
deployed application is available and is enabled by default. You can override the https://github.com/spring-cloud/spring-cloud-deployer-cloudfoundry#application-name-settings-and-deployments[default configurations]
and set the respective properties by using `cf set-env` commands.

For instance, if you want to disable the randomization, you can override it by using the following command:

====
[source]
----
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_ENABLE_RANDOM_APP_NAME_PREFIX false
----
====

[[configuration-custom-routes]]
== Custom Routes

As an alternative to a random name or to get even more control over the hostname used by the deployed apps, you can use
custom deployment properties, as the following example shows:

====
[source]
----
dataflow:>stream create foo --definition "http | log"

sdataflow:>stream deploy foo --properties "deployer.http.cloudfoundry.domain=mydomain.com,
                                          deployer.http.cloudfoundry.host=myhost,
                                          deployer.http.cloudfoundry.route-path=my-path"
----
====

The preceding example binds the `http` app to the `http://myhost.mydomain.com/my-path` URL. Note that this
example shows *all* of the available customization options. In practice, you can use only one or two out of the three.

[[configuration-docker-apps]]
== Docker Applications

Starting with version 1.2, it is possible to register and deploy Docker based apps as part of streams and tasks by using
Data Flow for Cloud Foundry.

If you use Spring Boot and RabbitMQ-based Docker images, you can provide a common deployment property
to facilitate binding the apps to the RabbitMQ service. Assuming your RabbitMQ service is named `rabbit`, you can provide the following:

====
[source]
----
cf set-env dataflow-server SPRING_APPLICATION_JSON '{"spring.cloud.dataflow.applicationProperties.stream.spring.rabbitmq.addresses": "${vcap.services.rabbit.credentials.protocols.amqp.uris}"}'
----
====

For Spring Cloud Task apps, you can use something similar to the following, if you use a database service instance named `mysql`:

====
[source]
----
cf set-env SPRING_DATASOURCE_URL '${vcap.services.mysql.credentials.jdbcUrl}'
cf set-env SPRING_DATASOURCE_USERNAME '${vcap.services.mysql.credentials.username}'
cf set-env SPRING_DATASOURCE_PASSWORD '${vcap.services.mysql.credentials.password}'
cf set-env SPRING_DATASOURCE_DRIVER_CLASS_NAME 'org.mariadb.jdbc.Driver'
----
====

For non-Java or non-Boot applications, your Docker app must parse the `VCAP_SERVICES` variable in order to bind to any available services.

[NOTE]
.Passing application properties
=====
When using non-Boot applications, chances are that you want to pass the application properties by using traditional
environment variables, as opposed to using the special `SPRING_APPLICATION_JSON` variable. To do so, set the
following variables for streams and tasks, respectively:

====
[source, properties]
----
SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_USE_SPRING_APPLICATION_JSON=false
SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_USE_SPRING_APPLICATION_JSON=false
----
====
=====

[[configuration-service-binding-at-application-level]]
== Application-level Service Bindings

When deploying streams in Cloud Foundry, you can take advantage of application-specific service bindings, so not all
services are globally configured for all the apps orchestrated by Spring Cloud Data Flow.

For instance, if you want to provide a `mysql` service binding only for the `jdbc` application in the following stream
definition, you can pass the service binding as a deployment property:

====
[source]
----
dataflow:>stream create --name httptojdbc --definition "http | jdbc"
dataflow:>stream deploy --name httptojdbc --properties "deployer.jdbc.cloudfoundry.services=mysqlService"
----


where `mysqlService` is the name of the service specifically bound only to the `jdbc` application and the `http`
application does not get the binding by this method.
====

If you have more than one service to bind, they can be passed as comma-separated items
(for example: `deployer.jdbc.cloudfoundry.services=mysqlService,someService`).

[[configuration-ups]]
== User-provided Services
In addition to marketplace services, Cloud Foundry supports
https://docs.cloudfoundry.org/devguide/services/user-provided.html[User-provided Services] (UPS). Throughout this reference manual,
regular services have been mentioned, but there is nothing precluding the use of User-provided Services as well, whether for use as the
messaging middleware (for example, if you want to use an external Apache Kafka installation) or for use by some
of the stream applications (for example, an Oracle Database).

Now we review an example of extracting and supplying the connection credentials from a UPS.

The following example shows a sample UPS setup for Apache Kafka:

====
[source,bash]
----
cf create-user-provided-service kafkacups -p '{‚Äùbrokers":"HOST:PORT","zkNodes":"HOST:PORT"}'
----
====

The UPS credentials are wrapped within `VCAP_SERVICES`, and they can be supplied directly in the stream definition, as
the following example shows.

====
[source]
----
stream create fooz --definition "time | log"
stream deploy fooz --properties "app.time.spring.cloud.stream.kafka.binder.brokers=${vcap.services.kafkacups.credentials.brokers},app.time.spring.cloud.stream.kafka.binder.zkNodes=${vcap.services.kafkacups.credentials.zkNodes},app.log.spring.cloud.stream.kafka.binder.brokers=${vcap.services.kafkacups.credentials.brokers},app.log.spring.cloud.stream.kafka.binder.zkNodes=${vcap.services.kafkacups.credentials.zkNodes}"
----
====

[[configuration-db-connection-pool]]
== Database Connection Pool
The Data Flow server uses the Spring Cloud Connector library to create the DataSource with a default connection pool size of 4. 
To change the connection pool size and maximum wait time, set the following two properties `spring.cloud.skipper.server.cloudfoundry.maxPoolSize` and `spring.cloud.skipper.server.cloudfoundry.maxWaitTime`. The wait time is specified in milliseconds.

[[configuration-maximum-disk-quota-configuration]]
== Maximum Disk Quota
By default, every application in Cloud Foundry starts with 1G disk quota and this can be adjusted to a default maximum of
2G. The default maximum can also be overridden up to 10G by using Pivotal Cloud Foundry's (PCF) Ops Manager GUI.

This configuration is relevant for Spring Cloud Data Flow because every stream and task deployment is composed of applications
(typically Spring Boot uber-jar's), and those applications are resolved from a remote maven repository. After resolution,
the application artifacts are downloaded to the local Maven Repository for caching and reuse. With this happening in the background,
the default disk quota (1G) can fill up rapidly, especially when we experiment with streams that
are made up of unique applications. In order to overcome this disk limitation and depending
on your scaling requirements, you may want to change the default maximum from 2G to 10G. Let's review the
steps to change the default maximum disk quota allocation.

=== PCF's Operations Manager

From PCF's Ops Manager, select the "`Pivotal Elastic Runtime`" tile and navigate to the "`Application Developer Controls`" tab.
Change the "`Maximum Disk Quota per App (MB)`" setting from 2048 (2G) to 10240 (10G). Save the disk quota update and click
"`Apply Changes`" to complete the configuration override.

[[configuration-scaling]]
== Scale Application

Once the disk quota change has been successfully applied and assuming you have a xref:running-on-cloudfoundry[running application],
you can scale the application with a new `disk_limit` through the CF CLI, as the following example shows:

====
[source,bash]
----
‚Üí cf scale dataflow-server -k 10GB

Scaling app dataflow-server in org ORG / space SPACE as user...
OK

....
....
....
....

     state     since                    cpu      memory           disk           details
#0   running   2016-10-31 03:07:23 PM   1.8%     497.9M of 1.1G   193.9M of 10G
----
====

You can then list the applications and see the new maximum disk space, as the following example shows:

====
[source,bash]
----
‚Üí cf apps
Getting apps in org ORG / space SPACE as user...
OK

name              requested state   instances   memory   disk   urls
dataflow-server   started           1/1         1.1G     10G    dataflow-server.apps.io
----
====

[[managing-disk-utilization]]
== Managing Disk Use

Even when configuring the Data Flow server to use 10G of space, there is the possibility of exhausting
the available space on the local disk.
If you deploy the Data Flow server by using the default `port` health check type, you must explicitly monitor the disk space on the server in order to avoid running out space.
If you deploy the server by using the `http` health check type (see the next example), the Data Flow server is restarted if there is low disk space.
This is due to Spring Boot's link:https://github.com/spring-projects/spring-boot/blob/v1.5.14.RELEASE/spring-boot-actuator/src/main/java/org/springframework/boot/actuate/health/DiskSpaceHealthIndicator.java[Disk Space Health Indicator].
You can link:https://docs.spring.io/spring-boot/docs/1.5.14.RELEASE/reference/htmlsingle/#common-application-properties[configure] the settings of the Disk Space Health Indicator by using the properties that have the `management.health.diskspace` prefix.

For version 1.7, we are investigating the use of link:https://docs.cloudfoundry.org/devguide/services/using-vol-services.html[Volume Services] for the Data Flow server to store `.jar` artifacts before pushing them to Cloud Foundry.

The following example shows how to deploy the `http` health check type to an endpoint called `/management/health`:

====
[source]
----
---
  ...
  health-check-type: http
  health-check-http-endpoint: /management/health
----
====

[[configuration-app-resolution-options]]
== Application Resolution Alternatives

Though we highly recommend using Maven Repository for application link:http://docs.spring.io/spring-cloud-dataflow/docs/{scdf-core-version}/reference/htmlsingle/#spring-cloud-dataflow-register-stream-apps[resolution and registration]
in Cloud Foundry, there might be situations where an alternative approach would make sense. The following alternative options
could help you resolve applications when running on Cloud Foundry.

* With the help of Spring Boot, we can serve link:https://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-developing-web-applications.html#boot-features-spring-mvc-static-content[static content]
in Cloud Foundry. A simple Spring Boot application can bundle all the required stream and task applications. By having it
run on Cloud Foundry, the static application can then serve the √ºber-jar's. From the shell, you can, for example, register the
application with the name `http-source.jar` by using `--uri=http://<Route-To-StaticApp>/http-source.jar`.

* The √ºber-jar's can be hosted on any external server that's reachable over HTTP. They can be resolved from raw GitHub URIs
as well. From the shell, you can, for example, register the app with the name `http-source.jar` by using `--uri=http://<Raw_GitHub_URI>/http-source.jar`.

* link:http://docs.cloudfoundry.org/buildpacks/staticfile/index.html[Static Buildpack] support in Cloud Foundry is another
option. A similar HTTP resolution works on this model, too.

* link:https://docs.cloudfoundry.org/devguide/services/using-vol-services.html[Volume Services] is another great option.
The required √ºber-jars can be hosted in an external file system. With the help of volume-services, you can, for
example, register the application with the name `http-source.jar` by using `--uri=file://<Path-To-FileSystem>/http-source.jar`.

[[getting-started-connection-pool]]
== Database Connection Pool
The Data Flow server uses the Spring Cloud Connector library to create the DataSource with a default connection pool size of `4`.
To change the connection pool size and maximum wait time, set the following two properties `spring.cloud.dataflow.server.cloudfoundry.maxPoolSize` and `spring.cloud.dataflow.server.cloudfoundry.maxWaitTime`.
The wait time is specified in milliseconds.

[[configuration-security]]
== Security

By default, the Data Flow server is unsecured and runs on an unencrypted HTTP connection. You can secure your REST endpoints
(as well as the Data Flow Dashboard) by enabling HTTPS and requiring clients to authenticate.
For more details about securing the
REST endpoints and configuring to authenticate against an OAUTH backend (UAA and SSO running on Cloud Foundry),
see the security section from the core http://docs.spring.io/spring-cloud-dataflow/docs/{scdf-core-version}/reference/htmlsingle/#configuration-security[reference guide]. You can configure the security details in `dataflow-server.yml` or pass them as environment variables through `cf set-env` commands.

[[getting-started-security-cloud-foundry]]
=== Authentication and Cloud Foundry

Spring Cloud Data Flow can either integrate with Pivotal Single Sign-On Service
(for example, on PWS) or Cloud Foundry User Account and Authentication (UAA) Server.

[[getting-started-security-cloud-foundry-sso]]
==== Pivotal Single Sign-On Service

When deploying Spring Cloud Data Flow to Cloud Foundry, you can bind the
application to the Pivotal Single Sign-On Service. By doing so, Spring Cloud
Data Flow takes advantage of the
https://github.com/pivotal-cf/spring-cloud-sso-connector[Spring Cloud Single Sign-On Connector],
which provides Cloud Foundry-specific auto-configuration support for OAuth 2.0.

To do so, bind the Pivotal Single Sign-On Service to your Data Flow Server application and
Single Sign-On (SSO) over OAuth2 will be enabled by default.

Authorization is similarly supported for non-Cloud Foundry security scenarios.
See the security section from the core Data Flow http://docs.spring.io/spring-cloud-dataflow/docs/{scdf-core-version}/reference/htmlsingle/#configuration-security[reference guide].

As the provisioning of roles can vary widely across environments, we by
default assign all Spring Cloud Data Flow roles to users.

You can customize this behavior by providing your own http://docs.spring.io/spring-boot/docs/current/api/org/springframework/boot/autoconfigure/security/oauth2/resource/AuthoritiesExtractor.html[`AuthoritiesExtractor`].

The following example shows one possible approach to set the custom `AuthoritiesExtractor` on the `UserInfoTokenServices`:

====
[source,java]
----
public class MyUserInfoTokenServicesPostProcessor
	implements BeanPostProcessor {

	@Override
	public Object postProcessBeforeInitialization(Object bean, String beanName) {
		if (bean instanceof UserInfoTokenServices) {
			final UserInfoTokenServices userInfoTokenServices = (UserInfoTokenServices) bean;
			userInfoTokenServices.setAuthoritiesExtractor(ctx.getBean(AuthoritiesExtractor.class));
		}
		return bean;
	}

	@Override
	public Object postProcessAfterInitialization(Object bean, String beanName) {
		return bean;
	}
}
----
====

Then you can declare it in your configuration class as follows:

====
[source,java]
----
@Bean
public BeanPostProcessor myUserInfoTokenServicesPostProcessor() {
	BeanPostProcessor postProcessor = new MyUserInfoTokenServicesPostProcessor();
	return postProcessor;
}
----
====

[[getting-started-security-cloud-foundry-uaa]]
==== Cloud Foundry UAA

The availability of Cloud Foundry User Account and Authentication (UAA) depends on the Cloud Foundry environment.
In order to provide UAA integration, you have to manually provide the necessary
OAuth2 configuration properties (for example, by setting the `SPRING_APPLICATION_JSON`
property).

The following JSON example shows how to create a security configuration:

====
[source,json]
----
{
  "security.oauth2.client.client-id": "scdf",
  "security.oauth2.client.client-secret": "scdf-secret",
  "security.oauth2.client.access-token-uri": "https://login.cf.myhost.com/oauth/token",
  "security.oauth2.client.user-authorization-uri": "https://login.cf.myhost.com/oauth/authorize",
  "security.oauth2.resource.user-info-uri": "https://login.cf.myhost.com/userinfo"
}
----
====

By default, the `spring.cloud.dataflow.security.cf-use-uaa`  property is set to `true`. This property activates a special
http://docs.spring.io/spring-boot/docs/current/api/org/springframework/boot/autoconfigure/security/oauth2/resource/AuthoritiesExtractor.html[`AuthoritiesExtractor`] called `CloudFoundryDataflowAuthoritiesExtractor`.

If you do not use CloudFoundry UAA, you should set `spring.cloud.dataflow.security.cf-use-uaa` to `false`.

Under the covers, this `AuthoritiesExtractor` calls out to the
https://apidocs.cloudfoundry.org/253/apps/retrieving_permissions_on_a_app.html[Cloud Foundry
Apps API] and ensure that users are in fact Space Developers.

If the authenticated user is verified as a Space Developer, all roles are assigned.
Otherwise, no roles whatsoever are assigned. In that case, you may see the following
Dashboard screen:

.Accessing the Data Flow Dashboard without Roles
image::cf-getting-started-security-no-roles.png[Dashboard without roles, scaledwidth="100%"]

== Configuration Reference

You must provide several pieces of configuration. These are Spring Boot `@ConfigurationProperties`, so you can set
them as environment variables or by any other means that Spring Boot supports. The following listing is in environment
variable format, as that is an easy way to get started configuring Boot applications in Cloud Foundry:

====
[source,bash]
----
# Default values appear after the equal signs.
# Example values, typical for Pivotal Web Services, are included as comments.

# URL of the CF API (used when using cf login -a for example) - for example, https://api.run.pivotal.io
# (to set the environment variable, use SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_URL).
spring.cloud.deployer.cloudfoundry.url=

# The name of the organization that owns the space above - for example, youruser-org
# (To set the environment variable, use SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_ORG).
spring.cloud.deployer.cloudfoundry.org=

# The name of the space into which modules will be deployed - for example, development
# (to set the environment variable, use SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SPACE).
spring.cloud.deployer.cloudfoundry.space=

# The root domain to use when mapping routes - for example, cfapps.io
# (to set the environment variable, use SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_DOMAIN).
spring.cloud.deployer.cloudfoundry.domain=

# The user name and password of the user to use to create applications
# (to set the environment variables, use SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_USERNAME
# and SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_PASSWORD).
spring.cloud.deployer.cloudfoundry.username=
spring.cloud.deployer.cloudfoundry.password=

# Whether to allow self-signed certificates during SSL validation (you should NOT do so in production)
# (to set the environment variable, use SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SKIP_SSL_VALIDATION).
spring.cloud.deployer.cloudfoundry.skipSslValidation=false

# The health check type to use for stream apps. Accepts 'none' and 'port'.
spring.cloud.deployer.cloudfoundry.stream.health-check=


# A comma-separated set of service instance names to bind to every deployed task application.
# Among other things, this should include an RDBMS service that is used
# for Spring Cloud Task execution reporting, such as my_mysql
# (to set the environment variable, use SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_SERVICES).
spring.cloud.deployer.cloudfoundry.task.services=

# Timeout, in seconds, to use when doing blocking API calls to Cloud Foundry
# (to set the  environment variable, use SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_API_TIMEOUT
# and SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_API_TIMEOUT).
spring.cloud.deployer.cloudfoundry.stream.apiTimeout=360
spring.cloud.deployer.cloudfoundry.task.apiTimeout=360

# Timeout, in milliseconds, to use when querying the Cloud Foundry API to compute app status
# (to set the environment variable, use SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_STATUS_TIMEOUT
# and SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_STATUS_TIMEOUT).
spring.cloud.deployer.cloudfoundry.stream.statusTimeout=5000
spring.cloud.deployer.cloudfoundry.task.statusTimeout=5000
----
====

Note that you can set `spring.cloud.deployer.cloudfoundry.services`,
`spring.cloud.deployer.cloudfoundry.buildpack`, or the Spring Cloud Deployer-standard
`spring.cloud.deployer.memory` and `spring.cloud.deployer.disk`
as part of an individual deployment request by using the `deployer.<app-name>` shortcut, as the following example shows:

====
[source]
----
stream create --name ticktock --definition "time | log"
stream deploy --name ticktock --properties "deployer.time.memory=2g"
----
====

The commands in the preceding example deploy the time source with 2048MB of memory, while the log sink uses the default 1024MB.

When you deploy a stream, you can also pass `JAVA_OPTS` as a deployment property, as the following example shows:

====
[source,bash]
----
stream deploy --name ticktock --properties "deployer.time.cloudfoundry.javaOpts=-Duser.timezone=America/New_York"
----
====

You can also set this property at the global level for all the streams as applicable to any deployment property by setting
`SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_JAVA_OPTS` as the server level property.

== Debugging
If you want to get better insights into what is happening when your streams and tasks are being deployed, you may want
to turn on the following features:

* Reactor "`stacktraces`", showing which operators were involved before an error occurred. This feature is helpful, as the deployer
relies on project reactor and regular stacktraces may not always allow understanding the flow before an error happened.
Note that this comes with a performance penalty, so it is disabled by default.
+
====
[source,bash]
----
spring.cloud.dataflow.server.cloudfoundry.debugReactor = true
----
====
* Deployer and Cloud Foundry client library request and response logs. This feature allows seeing a detailed conversation between
the Data Flow server and the Cloud Foundry Cloud Controller.
+
====
[source,data]
----
logging.level.cloudfoundry-client = DEBUG
----
====

== Spring Cloud Config Server
You can use Spring Cloud Config Server to centralize configuration properties for Spring Boot applications. Likewise,
both Spring Cloud Data Flow and the applications orchestrated by Spring Cloud Data Flow can be integrated with
a configuration server to use the same capabilities.

=== Stream, Task, and Spring Cloud Config Server
Similar to Spring Cloud Data Flow server, you can configure both the stream and task applications to resolve the centralized properties from the configuration server.
Setting the `spring.cloud.config.uri` property for the deployed applications is a common way to bind to the configuration server.
See the link:https://cloud.spring.io/spring-cloud-config/spring-cloud-config.html#_spring_cloud_config_client[Spring Cloud Config Client] reference guide for more information.
Since this property is likely to be used across all applications deployed by the Data Flow server, the Data Flow server's `spring.cloud.dataflow.applicationProperties.stream` property for stream applications and `spring.cloud.dataflow.applicationProperties.task` property for task applications can be used to pass the `uri` of the Config Server to each deployed stream or task application. See the section on http://docs.spring.io/spring-cloud-dataflow/docs/{scdf-core-version}/reference/htmlsingle/#spring-cloud-dataflow-global-properties[common application properties] for more information.

Note that, if you use applications from the link:http://cloud.spring.io/spring-cloud-stream-app-starters/[App Starters project], these applications already embed the `spring-cloud-services-starter-config-client` dependency.
If you build your application from scratch and want to add the client side support for config server, you can add a dependency reference to the config server client library. The following snippet shows a Maven example:

====
[source,xml]
----
...
<dependency>
  <groupId>io.pivotal.spring.cloud</groupId>
  <artifactId>spring-cloud-services-starter-config-client</artifactId>
  <version>CONFIG_CLIENT_VERSION</version>
</dependency>
...
----

where `CONFIG_CLIENT_VERSION` can be the latest release of the https://github.com/pivotal-cf/spring-cloud-services-connector/releases[Spring Cloud Config Server]
client for Pivotal Cloud Foundry.
====

NOTE: You may see a `WARN` logging message if the application that uses this library cannot connect to the configuration
server when the application starts and whenever the `/health` endpoint is accessed.
If you know that you are not using config server functionality, you can disable the client library by setting the
`SPRING_CLOUD_CONFIG_ENABLED` environment variable to `false`.
Another, more drastic, option is to disable the platform health check by setting the
`SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_HEALTH_CHECK` environment variable to `none`.

=== Sample Manifest Template

The following SCDF and Skipper `manifest.yml` templates includes the required environment variables for the Skipper and Spring Cloud Data Flow server and deployed applications and tasks to successfully run on Cloud Foundry and automatically resolve centralized properties from `my-config-server` at runtime:

====
[source,yml]
----
---
applications:
- name: data-flow-server
  host: data-flow-server
  memory: 2G
  disk_quota: 2G
  instances: 1
  path: {PATH TO SERVER UBER-JAR}
  env:
    SPRING_APPLICATION_NAME: data-flow-server
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_URL: https://api.local.pcfdev.io
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_ORG: pcfdev-org
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SPACE: pcfdev-space
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_DOMAIN: local.pcfdev.io
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_USERNAME: admin
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_PASSWORD: admin
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_SERVICES: mysql,my-config-server
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SKIP_SSL_VALIDATION: true
    SPRING_CLOUD_SKIPPER_CLIENT_SERVER_URI: https://<skipper-host-name>/api
    SPRING_APPLICATION_JSON: '{"maven": { "remote-repositories": { "repo1": { "url": "https://repo.spring.io/libs-release"} } } }'
services:
- mysql
- my-config-server

---
applications:
- name: skipper-server
  host: skipper-server
  memory: 1G
  disk_quota: 1G
  instances: 1
  timeout: 180
  buildpack: java_buildpack
  path: <PATH TO THE DOWNLOADED SKIPPER SERVER UBER-JAR>
  env:
    SPRING_APPLICATION_NAME: skipper-server
    SPRING_CLOUD_SKIPPER_SERVER_ENABLE_LOCAL_PLATFORM: false
    SPRING_CLOUD_SKIPPER_SERVER_STRATEGIES_HEALTHCHECK_TIMEOUTINMILLIS: 300000
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_CONNECTION_URL: https://api.local.pcfdev.io
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_CONNECTION_ORG: pcfdev-org
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_CONNECTION_SPACE: pcfdev-space
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_DEPLOYMENT_DOMAIN: cfapps.io
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_CONNECTION_USERNAME: admin
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_CONNECTION_PASSWORD: admin
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_CONNECTION_SKIP_SSL_VALIDATION: false
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_DEPLOYMENT_DELETE_ROUTES: false
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_DEPLOYMENT_SERVICES: rabbit, my-config-server
services:
- mysql
  my-config-server

----

where `my-config-server` is the name of the Spring Cloud Config Service instance running on Cloud Foundry.
====

By binding the service to Spring Cloud Data Flow server, Spring Cloud Task and via Skipper to all the Spring Cloud Stream applications respectively, we can now resolve centralized properties backed by this service.

=== Self-signed SSL Certificate and Spring Cloud Config Server

Often, in a development environment, we may not have a valid certificate to enable SSL communication between clients and the backend services.
However, the configuration server for Pivotal Cloud Foundry uses HTTPS for all client-to-service communication, so we need to add a self-signed SSL certificate in environments with no valid certificates.

By using the same `manifest.yml` templates listed in the previous section for the server, we can provide the self-signed SSL certificate by setting `TRUST_CERTS: <API_ENDPOINT>`.

However, the deployed applications also require `TRUST_CERTS` as a flat environment variable (as opposed to being wrapped inside `SPRING_APPLICATION_JSON`), so we must instruct the server with yet another set of tokens (`SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_USE_SPRING_APPLICATION_JSON: false` and `SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_USE_SPRING_APPLICATION_JSON: false`) for stream and task applications, respectively.
With this setup, the applications receive their application properties as regular environment variables.

The following listing shows the updated `manifest.yml` with the required changes. Both the Data Flow server and deployed applications
get their configuration from the `my-config-server` Cloud Config server (deployed as a Cloud Foundry service).

====
[source,yml]
----
---
applications:
- name: test-server
  host: test-server
  memory: 1G
  disk_quota: 1G
  instances: 1
  path: spring-cloud-dataflow-server-cloudfoundry-VERSION.jar
  env:
    SPRING_APPLICATION_NAME: test-server
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_URL: <URL>
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_ORG: <ORG>
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SPACE: <SPACE>
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_DOMAIN: <DOMAIN>
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_USERNAME: <USER>
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_PASSWORD: <PASSWORD>
    SPRING_CLOUD_SKIPPER_CLIENT_SERVER_URI: https://<skipper-host-name>/api
    MAVEN_REMOTE_REPOSITORIES_REPO1_URL: https://repo.spring.io/libs-release
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_SERVICES: config-server      #this for so all task applications bind to my-config-server
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_USE_SPRING_APPLICATION_JSON: false #this is for all the stream applications
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_USE_SPRING_APPLICATION_JSON: false #this is for all the task applications
    TRUST_CERTS: <API_ENDPOINT> #this is for the server
    spring.cloud.dataflow.applicationProperties.stream.TRUST_CERTS: <API_ENDPOINT> #this propagates to all streams
    spring.cloud.dataflow.applicationProperties.task.TRUST_CERTS: <API_ENDPOINT>   #this propagates to all tasks
services:
- mysql
- my-config-server #this is for the server
----
====

Also add the `my-config-server` service to the Skipper's manifest environment

====
[source,yml]
----
---
applications:
- name: skipper-server
  host: skipper-server
  memory: 1G
  disk_quota: 1G
  instances: 1
  timeout: 180
  buildpack: java_buildpack
  path: <PATH TO THE DOWNLOADED SKIPPER SERVER UBER-JAR>
  env:
    SPRING_APPLICATION_NAME: skipper-server
    SPRING_CLOUD_SKIPPER_SERVER_ENABLE_LOCAL_PLATFORM: false
    SPRING_CLOUD_SKIPPER_SERVER_STRATEGIES_HEALTHCHECK_TIMEOUTINMILLIS: 300000
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_CONNECTION_URL: <URL>
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_CONNECTION_ORG: <ORG>
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_CONNECTION_SPACE: <SPACE>
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_DEPLOYMENT_DOMAIN: <DOMAIN>
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_CONNECTION_USERNAME: <USER>
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_CONNECTION_PASSWORD: <PASSWORD>
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_DEPLOYMENT_SERVICES: rabbit, my-config-server #this is so all stream applications bind to my-config-server
services:
- mysql
  my-config-server

----
====

[[getting-started-scheduling-configuration]]
== Configure Scheduling
This section discusses how to configure Spring Cloud Data Flow to connect to the https://www.cloudfoundry.org/the-foundry/scheduler/[PCF-Scheduler] as its agent to execute tasks.

[NOTE]
====
Before following these instructions, be sure to have an instance of the PCF-Scheduler service running in your Cloud Foundry space.
To create a PCF-Scheduler in your space (assuming it is in your Market Place) execute the following from the CF CLI: `cf create-service scheduler-for-pcf standard <name of service>`.
Name of a service is later used to bound running application in _PCF_.
====

For scheduling, you must add (or update) the following environment variables in your environment:

* Enable scheduling for Spring Cloud Data Flow by setting `spring.cloud.dataflow.features.schedules-enabled` to `true`.
* Bind the task deployer to your instance of PCF-Scheduler by adding the PCF-Scheduler service name to the `SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_SERVICES` environment variable.
* Establish the URL to the PCF-Scheduler by setting the `SPRING_CLOUD_SCHEDULER_CLOUDFOUNDRY_SCHEDULER_URL` environment variable.

[NOTE]
====
After creating the preceding configurations, you must create any task definitions that need to be scheduled.
====

The following sample manifest shows both environment properties configured (assuming you have a PCF-Scheduler service available with the name `myscheduler`):

====
[source,yml]
----
---
applications:
- name: data-flow-server
  host: data-flow-server
  memory: 2G
  disk_quota: 2G
  instances: 1
  path: {PATH TO SERVER UBER-JAR}
  env:
    SPRING_APPLICATION_NAME: data-flow-server
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_URL: https://api.local.pcfdev.io
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_ORG: pcfdev-org
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SPACE: pcfdev-space
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_DOMAIN: local.pcfdev.io
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_USERNAME: admin
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_PASSWORD: admin
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_SERVICES: mysql,myscheduler
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SKIP_SSL_VALIDATION: true
    SPRING_CLOUD_DATAFLOW_FEATURES_SCHEDULES_ENABLED: true
    SPRING_CLOUD_SKIPPER_CLIENT_SERVER_URI: https://<skipper-host-name>/api
    SPRING_CLOUD_SCHEDULER_CLOUDFOUNDRY_SCHEDULER_URL: https://scheduler.local.pcfdev.io
    SPRING_APPLICATION_JSON: '{"maven": { "remote-repositories": { "repo1": { "url": "https://repo.spring.io/libs-release"} } } }'
services:
- mysql
----

Where the `SPRING_CLOUD_SCHEDULER_CLOUDFOUNDRY_SCHEDULER_URL` has the following format: `scheduler.<Domain-Name>` (for
example, `https://scheduler.local.pcfdev.io`). Check the actual address from your _PCF_ environment.
====

