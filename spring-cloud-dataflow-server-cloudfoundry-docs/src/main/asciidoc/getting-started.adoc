[[getting-started]]
= Getting started

== Deploying on Cloud Foundry

Spring Cloud Data Flow can be used to deploy modules in a Cloud Foundry environment. When doing so, the
server application can either run itself on Cloud Foundry, or on another installation (e.g. a simple laptop).

The required configuration amounts to the same in either case, and is merely related to providing credentials to the
Cloud Foundry instance so that the server can spawn applications itself. Any Spring Boot compatible configuration
mechanism can be used (passing program arguments, editing configuration files before building the application, using
link:https://github.com/spring-cloud/spring-cloud-config[Spring Cloud Config], using environment variables, etc.),
although some may prove more practicable than others when running _on_ Cloud Foundry.

NOTE: By default, the https://github.com/spring-cloud/spring-cloud-dataflow/tree/master/spring-cloud-dataflow-registry[application registry] in Spring Cloud Data Flow's Cloud Foundry server is empty. It is intentionally designed to allow users to have the flexibility of http://docs.spring.io/spring-cloud-dataflow/docs/{scdf-core-version}/reference/html/_dsl_syntax.html#_register_a_stream_app[choosing and registering] applications, as they find appropriate for the given use-case requirement. Depending on the message-binder of choice, users can register between http://repo.spring.io/libs-snapshot/org/springframework/cloud/stream/app/[RabbitMQ or Apache Kafka] based maven artifacts.

=== Provision a Redis service instance on Cloud Foundry
Use `cf marketplace` to discover which plans are available to you, depending on the details of your Cloud Foundry setup.
For example when using link:https://run.pivotal.io/[Pivotal Web Services]:

```
cf create-service rediscloud 30mb redis
```

A redis instance is required for analytics apps, and would typically be bound to such apps when you create an analytics
stream using the <<getting-started.adoc#getting-started-service-binding-at-application-level,per-app-binding>> feature.

=== Provision a Rabbit service instance on Cloud Foundry
Use `cf marketplace` to discover which plans are available to you, depending on the details of your Cloud Foundry setup.
For example when using link:https://run.pivotal.io/[Pivotal Web Services]:

```
cf create-service cloudamqp lemur rabbit
```

Rabbit is typically used as a messaging middleware between streaming apps and would be bound to each deployed app
thanks to the `SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_SERVICES` setting (see below).

=== Provision a MySQL service instance on Cloud Foundry
Use `cf marketplace` to discover which plans are available to you, depending on the details of your Cloud Foundry setup.
For example when using link:https://run.pivotal.io/[Pivotal Web Services]:

```
cf create-service cleardb spark my_mysql
```

An RDBMS is used to persist Data Flow state, such as stream definitions and deployment ids.
It can also be used for tasks to persist execution history.

=== Download the Spring Cloud Data Flow Server and Shell apps

[subs=attributes]
```
wget http://repo.spring.io/{version-type-lowercase}/org/springframework/cloud/spring-cloud-dataflow-server-cloudfoundry/{project-version}/spring-cloud-dataflow-server-cloudfoundry-{project-version}.jar
wget http://repo.spring.io/{dataflow-version-type-lowercase}/org/springframework/cloud/spring-cloud-dataflow-shell/{dataflow-project-version}/spring-cloud-dataflow-shell-{dataflow-project-version}.jar
```

=== Running the Server
You can either deploy the server application on Cloud Foundry itself or on your local machine.
The following two sections explain each way of running the server.

[[running-on-cloudfoundry]]
==== Deploying and Running the Server app on Cloud Foundry

Push the server application on Cloud Foundry, configure it (see below) and start it.

NOTE: You must use a unique name for your app; an app with the same name in the same organization will cause your
deployment to fail

[subs=attributes]
```
cf push dataflow-server -m 2G -k 2G --no-start -p spring-cloud-dataflow-server-cloudfoundry-{project-version}.jar
cf bind-service dataflow-server redis
cf bind-service dataflow-server my_mysql
```

IMPORTANT: The recommended minimal memory setting for the server is 2G. Also, to push apps to PCF and obtain
application property metadata, the server downloads applications to Maven repository hosted on the local disk.  While
you can specify up to 2G as a typical maximum value for disk space on a PCF installation, this can be increased to
10G.  Read the xref:getting-started-maximum-disk-quota-configuration[maximum disk quota] section for information on
how to configure this PCF property.  Also, the Data Flow server itself implements a Last Recently Used algorithm to
free disk space when it falls below a low water mark value.

NOTE: If you are pushing to a space with multiple users, for example on PWS, there may already be a route taken for the
applicaiton name you have chosen. You can use the options `--random-route` to avoid this when pushing the app.

Now we can configure the app. The following configuration is for Pivotal Web Services. You need to fill in \{org}, \{space},
\{email} and \{password} before running these commands.

```
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_URL https://api.run.pivotal.io
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_ORG {org}
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SPACE {space}
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_DOMAIN cfapps.io
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_SERVICES rabbit
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_SERVICES my_mysql
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_USERNAME {email}
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_PASSWORD {password}
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SKIP_SSL_VALIDATION false
```

WARNING: Only set 'Skip SSL Validation' to true if you're running on a Cloud Foundry instance using self-signed
certs (e.g. in development). Do not use for production.

NOTE: If you are deploying in an environment that requires you to sign on using the Pivotal Single Sign-On Service,
refer to the section <<getting-started-security-cloud-foundry>> for information on how to configure the server.

Spring Cloud Data Flow server implementations (be it for Cloud Foundry, Mesos, YARN, or Kubernetes) do not have
_any_ default remote maven repository configured. This is intentionally designed to provide the flexibility for
the users, so they can override and point to a remote repository of their choice. The out-of-the-box
applications that are supported by Spring Cloud Data Flow are available in Spring's repository,
so if you want to use them, you *must* set it as the remote repository as listed below.

```
cf set-env dataflow-server MAVEN_REMOTE_REPOSITORIES_REPO1_URL https://repo.spring.io/libs-snapshot
```
where `repo1` is an alias name for the remote repository.

[[configuring-defaults]]
===== Configuring Defaults for Deployed Apps
You can also set other optional properties that alter the way Spring Cloud Data Flow will deploy stream and task apps:

* The default memory and disk sizes for a deployed application can be configured. By default they are 1024 MB memory
and 1024 MB disk. To change these, as an example to 512 and 2048 respectively, use
+
```
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_MEMORY 512
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_DISK 2048
```

* The default number of instances to deploy is set to 1, but can be overridden using
+
```
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_INSTANCES 1
```

* You can set the buildpack that will be used to deploy each application. For example, to use the Java offline buildback,
set the following environment variable
+
```
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_BUILDPACK java_buildpack_offline
```

* The health check mechanism used by Cloud Foundry to assert if apps are running can be customized. Current supported options
are `port` (the default) and `none`. Change the default like so:
+
```
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_HEALTH_CHECK none
```

[NOTE]
====
These settings can be configured separately for stream and task apps. To alter settings for tasks, simply
substitute `STREAM` with `TASK` in the property name. As an example,

```
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_MEMORY 512
```
====

TIP: All the properties mentioned above are `@ConfigurationProperties` of the
Cloud Foundry deployer. See link:https://github.com/spring-cloud/spring-cloud-deployer-cloudfoundry/blob/{deployer-branch-or-tag}/src/main/java/org/springframework/cloud/deployer/spi/cloudfoundry/CloudFoundryDeploymentProperties.java[CloudFoundryDeploymentProperties.java] for more information.

We are now ready to start the app.

```
cf start dataflow-server
```

Alternatively, you can run the Admin application locally on your machine which is described in the next section.

[[sample-manifest-teample]]
==== Sample Manifest Template

As an alternative to setting environment variables via `cf set-env` command, you can curate all the relevant env-var's
in `manifest.yml` file and use `cf push` command to provision the server.

Following is a sample template to provision the server on PCFDev.

[source,bash,subs=attributes]
----
---
applications:
- name: <PREFERRED NAME OF THE SERVER APP>
  host: <PREFERRED HOST>
  memory: <PREFERRED MEMORY>
  disk_quota: <PREFERRED DISKQUOTA>
  timeout: <PREFERRED API TIMEOUT>
  instances: <NO OF INSTANCES>
  path: <ABSOLUTE PATH TO SERVER UBER-JAR> # (eg., /demo/spring-cloud-dataflow-server-cloudfoundry-1.1.0.RELEASE.jar)
  env:
    SPRING_APPLICATION_NAME: <PREFERRED NAME OF THE SERVER APP>
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_URL: https://api.local.pcfdev.io # (eg., URL for PCFDev)
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_ORG: pcfdev-org # (eg., ORG for PCFDev)
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SPACE: pcfdev-space # (eg., SPACE for PCFDev)
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_DOMAIN: local.pcfdev.io # (eg., DOMAIN for PCFDev)
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_USERNAME: admin # (eg., USER for PCFDev)
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_PASSWORD: admin # (eg., PASSWORD for PCFDev)
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_SERVICES: rabbit,redis # (eg., Services for Streams)
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_SERVICES: mysql # (eg., Services for Tasks)
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SKIP_SSL_VALIDATION: true
    MAVEN_REMOTE_REPOSITORIES_REPO1_URL: https://repo.spring.io/libs-snapshot
    JAVA_OPTS: '-Dlogging.level.cloudfoundry=DEBUG' # (eg., DEBUG logs for cf-java-client Calls)
services:
- mysql
- config-server
----

Once you're ready with the relevant properties in this file, you can issue `cf push` command from the directory where
this file is stored.

==== Running the Server app locally

To run the server application locally, targeting your Cloud Foundry installation, you you need to configure the
application either by passing in command line arguments (see below) or setting a number of environment variables.

To use environment variables set the following:

```
export SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_URL=https://api.run.pivotal.io
export SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_ORG={org}
export SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SPACE={space}
export SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_DOMAIN=cfapps.io
export SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_USERNAME={email}
export SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_PASSWORD={password}
export SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SKIP_SSL_VALIDATION=false

export SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_SERVICES=rabbit
# The following is for letting task apps write to their db.
# Note however that when the *server* is running locally, it can't access that db
# task related commands that show executions won't work then
export SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_SERVICES=my_mysql
```

You need to fill in \{org}, \{space}, \{email} and \{password} before running these commands.

WARNING: Only set 'Skip SSL Validation' to true if you're running on a Cloud Foundry instance using self-signed
certs (e.g. in development). Do not use for production.

Now we are ready to start the server application:

[subs=attributes]
```
java -jar spring-cloud-dataflow-server-cloudfoundry-{project-version}.jar [--option1=value1] [--option2=value2] [etc.]
```

TIP: Of course, all other parameterization options that were available when running the server _on_ Cloud Foundry are
still available. This is particularly true for xref:configuring-defaults[configuring defaults] for applications. Just
substitute `cf set-env` syntax with `export`.

NOTE: The current underlying PCF task capabilities are considered experimental for PCF version
versions less than 1.9.  See http://docs.spring.io/spring-cloud-dataflow/docs/{scdf-core-version}/reference/html/enable-disable-specific-features.html[Feature Togglers]
for how to disable task support in Data Flow.

=== Running Spring Cloud Data Flow Shell locally

Run the shell and optionally target the Admin application if not running on the same host (will typically be the case if
deployed on Cloud Foundry as explained xref:running-on-cloudfoundry[here])

[source,bash,subs=attributes]
----
$ java -jar spring-cloud-dataflow-shell-{dataflow-project-version}.jar
----

```
server-unknown:>dataflow config server http://dataflow-server.cfapps.io
Successfully targeted http://dataflow-server.cfapps.io
dataflow:>
```

By default, the application registry will be empty. If you would like to register all out-of-the-box stream applications
built with the RabbitMQ binder in bulk, you can with the following command. For more details, review how to
xref:spring-cloud-dataflow-register-apps[register applications].

```
dataflow:>app import --uri http://bit.ly/Avogadro-GA-stream-applications-rabbit-maven

```

[NOTE]
.A Note about application URIs
====
While Spring Cloud Data Flow for Cloud Foundry leverages the core Data Flow project, and as such theoretically supports
registering apps using any scheme, the use of `file://` URIs does not really make sense on Cloud Foundry. Indeed, the
local filesystem of the Data Flow server is ephemeral and chances are that you don't want to manually upload your apps there.

When deploying apps using Data Flow for Cloud Foundry, a typical choice is to use `maven://` coordinates, or maybe `http://` URIs.

Note that at the time of writing, Docker resources are not supported.
====

You can now use the shell commands to list available applications (source/processors/sink) and create streams. For example:

[source]
----
dataflow:> stream create --name httptest --definition "http | log" --deploy
----

NOTE: You will need to wait a little while until the apps are actually deployed successfully
before posting data.  Tail the log file for each application to verify
the application has started.

Now post some data. The URL will be unique to your deployment, the following is just an example
[source]
----
dataflow:> http post --target http://dataflow-AxwwAhK-httptest-http.cfapps.io --data "hello world"
----
Look to see if `hello world` ended up in log files for the `log` application.

To run a simple task application, you can register all the out-of-the-box task applications with the following command.

```
dataflow:>app import --uri http://bit.ly/Addison-GA-task-applications-maven

```

Now create a simple link:http://docs.spring.io/spring-cloud-task-app-starters/docs/1.0.1.RELEASE/reference/html/_timestamp_task.html[timestamp] task.

```
dataflow:>task create mytask --definition "timestamp --format='yyyy'"
```

Tail the logs, e.g. `cf logs mytask` and then launch the task in the UI or in the Data Flow Shell

```
dataflow:>task launch mytask
```

You will see the year `2017` printed in the logs. The execution status of the task is stored
in the database and you can retrieve information about the task execution using the shell commands
`task execution list` and `task execution status --id <ID_OF_TASK>` or though the Data Flow UI.


[[getting-started-security]]
== Security

By default, the Data Flow server is unsecured and runs on an unencrypted HTTP connection. You can secure your REST endpoints,
as well as the Data Flow Dashboard by enabling HTTPS and requiring clients to authenticate. More details about securing the
REST endpoints and configuring to authenticate against an OAUTH backend (_i.e: UAA/SSO running on Cloud Foundry_), please
review the security section from the core http://docs.spring.io/spring-cloud-dataflow/docs/{scdf-core-version}/reference/html/getting-started-security.html[reference guide]. The security configurations can be configured in `dataflow-server.yml` or passed as environment variables through `cf set-env` commands.

[[getting-started-security-cloud-foundry]]
=== Authentication and Cloud Foundry

When deploying Spring Cloud Data Flow to Cloud Foundry, you can take advantage of the
 https://github.com/pivotal-cf/spring-cloud-sso-connector[_Spring Cloud Single Sign-On Connector_],
 which provides Cloud Foundry specific auto-configuration support for OAuth 2.0,
 when used in conjunction with the _Pivotal Single Sign-On Service_.

Simply set `security.basic.enabled` to `true` and in Cloud Foundry bind the SSO
service to your Data Flow Server app and SSO will be enabled.

[[getting-started-app-names-cloud-foundry]]
== Application Names and Prefixes

To help avoid clashes with routes across spaces in Cloud Foundry, a naming strategy to provide a random prefix to a
deployed application is available and is enabled by default. The https://github.com/spring-cloud/spring-cloud-deployer-cloudfoundry#application-name-settings-and-deployments[default configurations]
are overridable and the respective properties can be set via `cf set-env` commands.

For instance, if you'd like to disable the randomization, you can override it through:

```
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_ENABLE_RANDOM_APP_NAME_PREFIX false
```

=== Using Custom Routes

As an alternative to random name, or to get even more control over the hostname used by the deployed apps, one can use
custom deployment properties, as such:

[[source]
----
dataflow:>stream create foo --definition "http | log"

dataflow:>stream deploy foo --properties "app.http.spring.cloud.deployer.cloudfoundry.domain=mydomain.com,
                                          app.http.spring.cloud.deployer.cloudfoundry.host=myhost,
                                          app.http.spring.cloud.deployer.cloudfoundry.route-path=my-path"
----

This would result in the `http` app being bound to the URL `http://myhost.mydomain.com/my-path`. Note that this is an
example showing *all* customization options available. One can of course only leverage one or two out of the three.

== Configuration Reference

The following pieces of configuration must be provided. These are Spring Boot `@ConfigurationProperties` so you can set
them as environment variables or by any other means that Spring Boot supports.  Here is a listing in environment
variable format as that is an easy way to get started configuring Boot applications in Cloud Foundry.

```
# Default values cited after the equal sign.
# Example values, typical for Pivotal Web Services, cited as a comment

# url of the CF API (used when using cf login -a for example), e.g. https://api.run.pivotal.io
# (for setting env var use SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_URL)
spring.cloud.deployer.cloudfoundry.url=

# name of the organization that owns the space above, e.g. youruser-org
# (For Setting Env var use SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_ORG)
spring.cloud.deployer.cloudfoundry.org=

# name of the space into which modules will be deployed, e.g. development
# (for setting env var use SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SPACE)
spring.cloud.deployer.cloudfoundry.space=

# the root domain to use when mapping routes, e.g. cfapps.io
# (for setting env var use SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_DOMAIN)
spring.cloud.deployer.cloudfoundry.domain=

# username and password of the user to use to create apps
# (for setting env var use SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_USERNAME and SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_PASSWORD)
spring.cloud.deployer.cloudfoundry.username=
spring.cloud.deployer.cloudfoundry.password=

# Whether to allow self-signed certificates during SSL validation
# (for setting env var use SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SKIP_SSL_VALIDATION)
spring.cloud.deployer.cloudfoundry.skipSslValidation=false

# Comma separated set of service instance names to bind to every stream app deployed.
# Amongst other things, this should include a service that will be used
# for Spring Cloud Stream binding, e.g. rabbit
# (for setting env var use SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_SERVICES)
spring.cloud.deployer.cloudfoundry.stream.services=

# Health check type to use for stream apps. Accepts 'none' and 'port'
spring.cloud.deployer.cloudfoundry.stream.health-check=


# Comma separated set of service instance names to bind to every task app deployed.
# Amongst other things, this should include an RDBMS service that will be used
# for Spring Cloud Task execution reporting, e.g. my_mysql
# (for setting env var use SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_SERVICES)
spring.cloud.deployer.cloudfoundry.task.services=

# Timeout to use, in seconds, when doing blocking API calls to Cloud Foundry.
# (for setting env var use SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_API_TIMEOUT
and SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_API_TIMEOUT)
spring.cloud.deployer.cloudfoundry.stream.apiTimeout=360
spring.cloud.deployer.cloudfoundry.task.apiTimeout=360
```

Note that you can set the following properties `spring.cloud.deployer.cloudfoundry.services`,
`spring.cloud.deployer.cloudfoundry.buildpack` or the Spring Cloud Deployer standard
`spring.cloud.deployer.memory` and `spring.cloud.deployer.disk`
as part of an individual deployment request prefixed by the `app.<name of application>`. For example

```
>stream create --name ticktock --definition "time | log"
>stream deploy --name ticktock --properties "app.time.spring.cloud.deployer.memory=2g"
```

will deploy the time source with 2048MB of memory, while the log sink will use the default 1024MB.

=== Understanding what's going on
If you want to get better insights into what is happening when your streams and tasks are being deployed, you may want
to turn on the following features:

* Reactor "stacktraces", showing which operators were involved before an error occurred. This is helpful as the deployer
relies on project reactor and regular stacktraces may not always allow understanding the flow before an error happened.
Note that this comes with a performance penalty, so is disabled by default.
+
```
spring.cloud.dataflow.server.cloudfoundry.debugReactor = true
```
* Deployer and Cloud Foundry client library request/response logs. This allows seeing detailed conversation between
the Data Flow server and the Cloud Foundry Cloud Controller.
+
```
logging.level.cloudfoundry-client = DEBUG

```

=== Using Spring Cloud Config Server
Spring Cloud Config Server can be used to centralize configuration properties for Spring Boot applications. Likewise,
both Spring Cloud Data Flow and the applications orchestrated using Spring Cloud Data Flow can be integrated with
config-server to leverage the same capabilities. Let's review the integration steps for both the variants.

==== Spring Cloud Data Flow and Spring Cloud Config Server

* Download 1.4.x release of Spring Boot from http://start.spring.io
* Add `spring-cloud-dataflow-server-cloudfoundry-autoconfig` dependency pointing to Spring Cloud Data Flow's Cloud Foundry link:https://github.com/spring-cloud/spring-cloud-dataflow-server-cloudfoundry/releases[release]
* Add `@EnableDataFlowServer` to the downloaded application

[source,java]
----
@SpringBootApplication
@EnableDataFlowServer
public class DataFlowServer {

	public static void main(String[] args) {
		new SpringApplication(DataFlowServer.class).run(args);
	}
}
----

* Add  `spring-cloud-services-starter-config-client` dependency in `pom.xml`. A maven example follows.

[source,xml]
----
<dependency>
  <groupId>org.springframework.cloud</groupId>
  <artifactId>spring-cloud-dataflow-server-cloudfoundry-autoconfig</artifactId>
  <version>CF_SERVER_VERSION</version>
</dependency>
<dependency>
  <groupId>io.pivotal.spring.cloud</groupId>
  <artifactId>spring-cloud-services-starter-config-client</artifactId>
  <version>CONFIG_CLIENT_VERSION</version>
</dependency>
----

Where, `CF_SERVER_VERSION` and `CONFIG_CLIENT_VERSION` tokens can be the latest release versions of SCDF's link:https://github.com/spring-cloud/spring-cloud-dataflow-server-cloudfoundry/releases[CF-server]
and link:https://github.com/pivotal-cf/spring-cloud-services-connector/releases[Spring Cloud Config Server] client for
Pivotal Cloud Foundry respectively.

* Build the application locally

This completes the custom build of Spring Cloud Data Flow with `spring-cloud-services-starter-config-client`
library included in it as a dependency.

The final über-jar is now ready to be deployed to Cloud Foundry. With this setup and having the deployed application
bound to config-server service on Cloud Foundry, we can successfully negotiate, read, and resolve centralized properties
at the runtime.

Follow the http://docs.pivotal.io/spring-cloud-services/config-server[documentation]
for Config Server for Pivotal Cloud Foundry. For more details, please refer to Spring Cloud Services
http://docs.pivotal.io/spring-cloud-services/client-dependencies.html#config-server[client-dependencies documentation].

==== Stream, Task, and Spring Cloud Config Server
Similar to Spring Cloud Data Flow server, it is also possible to configure both the stream and task applications to
resolve the centralized properties from config-server.

Let's assume you'd like to read properties for `time-source` application from config-server.

* Download the `time-source` application starter with "Rabbit binder starter" from http://start-scs.cfapps.io/
* Load the downloaded project in an IDE
* Add `@Import(org.springframework.cloud.stream.app.time.source.TimeSourceConfiguration.class)` to import time-source's
configuration properties
* Add  `spring-cloud-services-starter-config-client` dependency

* Build the application locally

This completes the custom build of `time-source` application with `spring-cloud-services-starter-config-client`
library included in it as a dependency. A maven example follows.

[source,xml]
----
<dependency>
  <groupId>org.springframework.cloud.stream.app</groupId>
  <artifactId>spring-cloud-starter-stream-source-time</artifactId>
</dependency>
<dependency>
  <groupId>org.springframework.cloud</groupId>
  <artifactId>spring-cloud-starter-stream-rabbit</artifactId>
</dependency>
<dependency>
  <groupId>io.pivotal.spring.cloud</groupId>
  <artifactId>spring-cloud-services-starter-config-client</artifactId>
  <version>CONFIG_CLIENT_VERSION</version>
</dependency>
----

Where, `CONFIG_CLIENT_VERSION` can be the latest release of link:https://github.com/pivotal-cf/spring-cloud-services-connector/releases[Spring Cloud Config Server]
client for Pivotal Cloud Foundry.

The final `time-source` über-jar is now ready to be registered in Spring Cloud Data Flow.  For more details, review how
to xref:spring-cloud-dataflow-register-apps[register applications]. With this setup and having the
deployed application bound to config-server service on Cloud Foundry, we can successfully negotiate, read, and resolve
centralized properties at the runtime.

[NOTE]
When deploying apps using Data Flow for Cloud Foundry, a typical choice is to use `maven://` coordinates, or maybe `http://` URIs.

==== Sample Manifest Template
Following `manifest.yml` template includes the required env-var's for the Spring Cloud Data Flow server to successfully
run on Cloud Foundry and automatically resolve centralized properties from config-server at the runtime.

[source,yml]
----
---
applications:
- name: test-server
  host: test-server
  memory: 1G
  disk_quota: 1G
  instances: 1
  path: spring-cloud-dataflow-server-cloudfoundry-VERSION.jar
  env:
    SPRING_APPLICATION_NAME: test-server
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_URL: <URL>
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_ORG: <ORG>
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SPACE: <SPACE>
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_DOMAIN: <DOMAIN>
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_USERNAME: <USER>
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_PASSWORD: <PASSWORD>
    MAVEN_REMOTE_REPOSITORIES_REPO1_URL: https://repo.spring.io/libs-release
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_SERVICES: config-server #this is for all the stream applications
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_SERVICES: config-server #this is for all the task applications
services:
- mysql
- config-server #this is for the server
----

Where, `config-server` is the name of the Spring Cloud Config Service instance running on Cloud Foundry. By binding the
service to both Spring Cloud Data Flow server and as well as all the Spring Cloud Stream and Spring Cloud Task applications
through `SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_SERVICES: config-server` and `SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_SERVICES: config-server`
respectively, we can now resolve centralized properties backed by this service.

==== Self-signed SSL Certificate and Spring Cloud Config Server
Often, in a development environment, we may not have a valid certificate to enable SSL communication between clients and
the backend services. However, the config-server for Pivotal Cloud Foundry uses HTTPS for all client-to-service communication,
so it is necessary to add a self-signed SSL certificate in environments with no valid certificates.

Using the same `manifest.yml` template listed in the previous section, for the server, we can provide the self-signed
SSL certificate via: `TRUST_CERTS: <API_ENDPOINT>`.

For Spring Cloud Stream and Spring Cloud Task applications, it is necessary to wrap the `TRUST_CERTS` in `SPRING_APPLICATION_JSON`
token - this instructs the server to propagate `SPRING_APPLICATION_JSON` content to all the deployed applications.

However, the deployed applications require `TRUST_CERTS` as a _flat env-var_ as opposed to being wrapped inside
`SPRING_APPLICATION_JSON`, so we will have to instruct the server with yet another set of tokens `SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_USE_SPRING_APPLICATION_JSON: false`
and `SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_USE_SPRING_APPLICATION_JSON: false` for stream and task applications
respectively. With this setup, the applications will be deployed with the content in `SPRING_APPLICATION_JSON` as
flat env-var's.

Let's review the updated `manifest.yml` with the required changes.

[source,yml]
----
---
applications:
- name: test-server
  host: test-server
  memory: 1G
  disk_quota: 1G
  instances: 1
  path: spring-cloud-dataflow-server-cloudfoundry-VERSION.jar
  env:
    SPRING_APPLICATION_NAME: test-server
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_URL: <URL>
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_ORG: <ORG>
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SPACE: <SPACE>
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_DOMAIN: <DOMAIN>
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_USERNAME: <USER>
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_PASSWORD: <PASSWORD>
    MAVEN_REMOTE_REPOSITORIES_REPO1_URL: https://repo.spring.io/libs-release
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_SERVICES: config-server #this is for all the stream applications
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_SERVICES: config-server #this is for all the task applications
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_USE_SPRING_APPLICATION_JSON: false #this is for all the stream applications
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_USE_SPRING_APPLICATION_JSON: false #this is for all the task applications
    TRUST_CERTS: <API_ENDPOINT> #this is for the server
    SPRING_APPLICATION_JSON: '{"spring.cloud.dataflow.applicationProperties.stream.TRUST_CERTS" : <API_ENDPOINT>,"spring.cloud.dataflow.applicationProperties.task.TRUST_CERTS" : <API_ENDPOINT>}'
services:
- mysql
- config-server #this is for the server
----

[[getting-started-service-binding-at-application-level]]
== Application Level Service Bindings
When deploying streams in Cloud Foundry, you can take advantage of application specific service bindings, so not all
services are globally configured for all the apps orchestrated by Spring Cloud Data Flow.

For instance, if you'd like to provide `mysql` service binding only for the `jdbc` application in the following stream
definition, you can pass the service binding as a deployment property.

[source]
----
dataflow:>stream create --name httptojdbc --definition "http | jdbc"
dataflow:>stream deploy --name httptojdbc --properties "app.jdbc.spring.cloud.deployer.cloudfoundry.services=mysqlService"
----

Where, `mysqlService` is the name of the service specifically only bound to `jdbc` application and the `http`
application wouldn't get the binding by this method. If you have more than one service to bind, they can be passed as comma separated items (_eg: app.jdbc.spring.cloud.deployer.cloudfoundry.services=mysqlService,someService_).

[[getting-started-ups]]
== A Note About User Provided Services
In addition to marketplace services, Cloud Foundry supports
https://docs.cloudfoundry.org/devguide/services/user-provided.html[User Provided Services] (UPS). Throughout this reference manual,
regular services have been mentioned, but there is nothing precluding the use of UPSs as well, whether for use as the
messaging middleware (_e.g._ if you'd like to use an external Apache Kafka installation) or for _ad hoc_ usage by some
 of the stream apps (_e.g._ an Oracle Database).

Let's review an example of extracting and supplying the connection credentials from an UPS.

* A sample UPS setup for Apache Kafka.

[source,bash]
----
cf create-user-provided-service kafkacups -p '{”brokers":"HOST:PORT","zkNodes":"HOST:PORT"}'
----

* The UPS credentials will be wrapped within `VCAP_SERVICES` and it can be supplied directly in the stream definition like
the following.

[source]
----
stream create fooz --definition "time | log"
stream deploy fooz --properties "app.time.spring.cloud.stream.kafka.binder.brokers=${vcap.services.kafkacups.credentials.brokers},app.time.spring.cloud.stream.kafka.binder.zkNodes=${vcap.services.kafkacups.credentials.zkNodes},app.log.spring.cloud.stream.kafka.binder.brokers=${vcap.services.kafkacups.credentials.brokers},app.log.spring.cloud.stream.kafka.binder.zkNodes=${vcap.services.kafkacups.credentials.zkNodes}"
----

[[getting-started-service-application-rolling-upgrades]]
== Application Rolling Upgrades
Similar to Cloud Foundry's https://docs.pivotal.io/pivotalcf/1-7/devguide/deploy-apps/blue-green.html[blue-green] deployments,
you can perform rolling upgrades on the applications orchestrated by Spring Cloud Data Flow.

Let's start with the following simple stream definition.

[source]
----
dataflow:>stream create --name foo --definition "time | log" --deploy
----

List Apps.

[source,bash]
----
→ cf apps
Getting apps in org test-org / space development as test@pivotal.io...
OK

name       requested state   instances   memory   disk   urls
foo-log    started           1/1         1G       1G     foo-log.cfapps.io
foo-time   started           1/1         1G       1G     foo-time.cfapps.io
----

Let's assume you've to make an enhancement to update the "logger" to append extra text in every log statement.

* Download the `Log Sink` application starter with "Rabbit binder starter" from http://start-scs.cfapps.io/
* Load the downloaded project in an IDE
* Import the `LogSinkConfiguration.class`
* Adapt the handler to add extra text: `loggingHandler.setLoggerName("TEST [" + this.properties.getName() + "]");`
* Build the application locally

[source,java]
----
@SpringBootApplication
@Import(LogSinkConfiguration.class)
public class DemoApplication {

	@Autowired
	private LogSinkProperties properties;

	public static void main(String[] args) {
		SpringApplication.run(DemoApplication.class, args);
	}

	@Bean
	@ServiceActivator(inputChannel = Sink.INPUT)
	public LoggingHandler logSinkHandler() {
		LoggingHandler loggingHandler = new LoggingHandler(this.properties.getLevel().name());
		loggingHandler.setExpression(this.properties.getExpression());
		loggingHandler.setLoggerName("TEST [" + this.properties.getName() + "]");
		return loggingHandler;
	}
}
----

Let's deploy the locally built application to Cloud Foundry

[source,bash]
----
→ cf push foo-log-v2 -p demo-0.0.1-SNAPSHOT.jar -n foo-log-v2 --no-start
----

List Apps.

[source,bash]
----
→ cf apps
Getting apps in org test-org / space development as test@pivotal.io...
OK

name       requested state   instances   memory   disk   urls
foo-log    started           1/1         1G       1G     foo-log.cfapps.io
foo-time   started           1/1         1G       1G     foo-time.cfapps.io
foo-log-v2 stopped           1/1         1G       1G     foo-log-v2.cfapps.io
----

The stream applications do not communicate via (Go)Router, so they aren't generating HTTP traffic. Instead, they
communicate via the underlying messaging middleware such as Kafka or RabbitMQ. In order to rolling upgrade to route the
payload from old to the new version of the application, you'd have to replicate the `SPRING_APPLICATION_JSON` environment
variable from the old application that includes `spring.cloud.stream.bindings.input.destination` and `spring.cloud.stream.bindings.input.group` credentials.

NOTE: You can find the `SPRING_APPLICATION_JSON` of the old application via: `"cf env foo-log"`.

[source,bash]
----
cf set-env foo-log-v2 SPRING_APPLICATION_JSON '{"spring.cloud.stream.bindings.input.destination":"foo.time","spring.cloud.stream.bindings.input.group":"foo"}'
----

Let's start `foo-log-v2` application.

[source,bash]
----
cf start foo-log-v2
----

As soon as the application bootstraps, you'd now notice the payload being load balanced between two log application
instances running on Cloud Foundry. Since they both share the same "destination" and "consumer group", they are now
acting as competing consumers.

Old App Logs:

[source]
----
2016-08-08T17:11:08.94-0700 [APP/0]      OUT 2016-08-09 00:11:08.942  INFO 19 --- [ foo.time.foo-1] log.sink                                 : 08/09/16 00:11:08
2016-08-08T17:11:10.95-0700 [APP/0]      OUT 2016-08-09 00:11:10.954  INFO 19 --- [ foo.time.foo-1] log.sink                                 : 08/09/16 00:11:10
2016-08-08T17:11:12.94-0700 [APP/0]      OUT 2016-08-09 00:11:12.944  INFO 19 --- [ foo.time.foo-1] log.sink                                 : 08/09/16 00:11:12
----

New App Logs:

[source]
----
2016-08-08T17:11:07.94-0700 [APP/0]      OUT 2016-08-09 00:11:07.945  INFO 26 --- [ foo.time.foo-1] TEST [log.sink                       : 08/09/16 00:11:07]
2016-08-08T17:11:09.92-0700 [APP/0]      OUT 2016-08-09 00:11:09.925  INFO 26 --- [ foo.time.foo-1] TEST [log.sink                       : 08/09/16 00:11:09]
2016-08-08T17:11:11.94-0700 [APP/0]      OUT 2016-08-09 00:11:11.941  INFO 26 --- [ foo.time.foo-1] TEST [log.sink                       : 08/09/16 00:11:11]
----

Deleting the old version `foo-log` from the CF CLI would make all the payload consumed by the `foo-log-v2` application. Now,
you've successfully upgraded an application in the streaming pipeline without bringing it down in entirety to do
an adjustment in it.

List Apps.

[source,bash]
----
→ cf apps
Getting apps in org test-org / space development as test@pivotal.io...
OK

name       requested state   instances   memory   disk   urls
foo-time   started           1/1         1G       1G     foo-time.cfapps.io
foo-log-v2 started           1/1         1G       1G     foo-log-v2.cfapps.io
----

NOTE: A comprehensive canary analysis along with rolling upgrades will be supported via http://www.spinnaker.io/[Spinnaker]
in future releases.

[[getting-started-maximum-disk-quota-configuration]]
== Maximum Disk Quota Configuration
By default, every application in Cloud Foundry starts with 1G disk quota and this can be adjusted to a default maximum of
2G. The default maximum can also be overridden up to 10G via Pivotal Cloud Foundry's (PCF) Ops Manager GUI.

This configuration is relevant for Spring Cloud Data Flow because every stream and task deployment is composed of applications
(typically Spring Boot uber-jar's) and those applications are resolved from a remote maven repository. After resolution,
the application artifacts are downloaded to the local Maven Repository for caching/reuse. With this happening in the background,
there is a possibility the default disk quota (_1G_) fills up rapidly; especially, when we are experimenting with streams that
are made up of unique applications.  In order to overcome this disk limitation and depending
on your scaling requirements,you may want to change the default maximum from 2G to 10G. Let's review the
steps to change the default maximum disk quota allocation.

=== PCF's Operations Manager Configuration

From PCF's Ops Manager, Select "*Pivotal Elastic Runtime*" tile and navigate to "*Application Developer Controls*" tab.
Change the "*Maximum Disk Quota per App (MB)*" setting from 2048 to 10240 (_10G_). Save the disk quota update and hit
"Apply Changes" to complete the configuration override.

=== Scale Application

Once the disk quota change is applied successfully and assuming you've a xref:running-on-cloudfoundry[running application],
you may scale the application with a new `disk_limit` through CF CLI.

[source,bash]
----
→ cf scale dataflow-server -k 10GB

Scaling app dataflow-server in org ORG / space SPACE as user...
OK

....
....
....
....

     state     since                    cpu      memory           disk           details
#0   running   2016-10-31 03:07:23 PM   1.8%     497.9M of 1.1G   193.9M of 10G
----

[source,bash]
----
→ cf apps
Getting apps in org ORG / space SPACE as user...
OK

name              requested state   instances   memory   disk   urls
dataflow-server   started           1/1         1.1G     10G    dataflow-server.apps.io
----

=== Configuring target free disk percentage

Even when configuring the Data Flow server to use 10G of space, there is the possibility of exhausting
the available space on the local disk.  The server implements a least recently used (LRU) algorithm that
will remove maven artifacts from the local maven repository.  This is configured using the following
configuration property, the default value is 25.

[source]
----
# The low water mark percentage, expressed as in integer between 0 and 100, that triggers cleanup of
# the local maven repository
# (for setting env var use SPRING_CLOUD_DATAFLOW_SERVER_CLOUDFOUNDRY_FREE_DISK_SPACE_PERCENTAGE)
spring.cloud.dataflow.server.cloudfoundry.freeDiskSpacePercentage=25
----
