[[getting-started]]
= Getting started

[[getting-started-requirements]]
== System Requirements

The Spring Cloud Data Flow server deploys streams (long-lived applications) and tasks (short-lived applications) to Cloud
Foundry. The server is a lightweight Spring Boot application and it can run on Cloud Foundry or your laptop, but it is
common to run the server in Cloud Foundry.

Spring Cloud Data Flow requires a few data services to perform streaming, task/batch processing, and analytics. There are
two options to provisioning Spring Cloud Data Flow and related services on Cloud Foundry.

- The simplest and the automated method is to use the link:https://network.pivotal.io/products/p-dataflow[Spring Cloud Data Flow for PCF]
tile. This is an opinionated tile for Pivotal Cloud Foundry and the tile automatically provisions the server and the required
data services, thus simplifying the overall getting-started experience. You can read more about the installation
link:http://docs.pivotal.io/scdf/[here].
- Alternatively, you can provision all the components manually. The following section goes into the specifics on how it
is done.

=== Provision a Redis service instance on Cloud Foundry
A redis instance is required for analytics apps, and would typically be bound to such apps when you create an analytics
stream using the <<getting-started.adoc#getting-started-service-binding-at-application-level,per-app-binding>> feature.

Use `cf marketplace` to discover which plans are available to you, depending on the details of your Cloud Foundry setup.
For example when using link:https://run.pivotal.io/[Pivotal Web Services]:

```
cf create-service rediscloud 30mb redis
```

=== Provision a Rabbit service instance on Cloud Foundry
RabbitMQ is used as a messaging middleware between streaming apps and would be bound to each deployed streaming
apps. Apache Kafka is the other option. We can use `SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_SERVICES` setting in Data
Flow configuration or `SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_DEPLOYMENT_SERVICES` setting in
Skipper, which will automatically bind RabbitMQ to the deployed streaming applications.

Use `cf marketplace` to discover which plans are available to you, depending on the details of your Cloud Foundry setup.
For example when using link:https://run.pivotal.io/[Pivotal Web Services]:

```
cf create-service cloudamqp lemur rabbit
```

=== Provision a MySQL service instance on Cloud Foundry
An RDBMS is used to persist Data Flow state, such as stream/task definitions, deployments, and executions.

Use `cf marketplace` to discover which plans are available to you, depending on the details of your Cloud Foundry setup.
For example when using link:https://run.pivotal.io/[Pivotal Web Services]:

```
cf create-service cleardb spark my_mysql
```

[[getting-started-cloudfoundry]]
== Cloud Foundry Installation
Starting 1.3.x, the Data Flow Server can run in either `skipper` or `classic` (non-skipper) modes. The modes can be
specified when starting the Data Flow server using the property `spring.cloud.dataflow.features.skipper-enabled`.
By default, the `classic` mode is enabled.

. Download the Data Flow server and shell applications
+
[subs=attributes]
```
wget http://repo.spring.io/{version-type-lowercase}/org/springframework/cloud/spring-cloud-dataflow-server-cloudfoundry/{project-version}/spring-cloud-dataflow-server-cloudfoundry-{project-version}.jar

wget http://repo.spring.io/{dataflow-version-type-lowercase}/org/springframework/cloud/spring-cloud-dataflow-shell/{dataflow-project-version}/spring-cloud-dataflow-shell-{dataflow-project-version}.jar
```
. Optionally, download http://cloud.spring.io/spring-cloud-skipper/[Skipper] if you want the added features of upgrading and rolling back Streams since Data Flow delegates to Skipper for those features.
+
[source,yaml,options=nowrap,subs=attributes]
----
wget http://repo.spring.io/{skipper-version-type-lowercase}/org/springframework/cloud/spring-cloud-skipper-server/{skipper-version}/spring-cloud-skipper-server-{skipper-version}.jar
----
+
Push Skipper to Cloud Foundry only if you want to run Spring Cloud Data Flow server in `skipper` mode.  Here is a sample
manifest for Skipper.
+
[source,yaml,options=nowrap]
----
---
applications:
- name: skipper-server
  host: skipper-server
  memory: 1G
  disk_quota: 1G
  instances: 1
  timeout: 180
  path: <PATH TO THE DOWNLOADED SKIPPER SERVER UBER-JAR>
env:
    SPRING_APPLICATION_NAME: skipper-server
    SPRING_CLOUD_SKIPPER_SERVER_ENABLE_LOCAL_PLATFORM: false
    SPRING_CLOUD_SKIPPER_SERVER_STRATEGIES_HEALTHCHECK.TIMEOUTINMILLIS: 300000
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_CONNECTION_URL: https://api.run.pivotal.io
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_CONNECTION_ORG: {org}
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_CONNECTION_SPACE: {space}
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_CONNECTION_DOMAIN: cfapps.io
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_CONNECTION_USERNAME: {email}
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_CONNECTION_PASSWORD: {password}
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_DEPLOYMENT_SERVICES: {middlewareServiceName}
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_CONNECTION_SKIP_SSL_VALIDATION: false
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_CONNECTION_STREAM_ENABLE_RANDOM_APP_NAME_PREFIX: false
----

+
You need to fill in \{org}, \{space}, \{email},  \{password} and {middlewareServiceName} before running these commands. Once you have the desired
config values in the `manifest.yml`, you can run `cf push` command to provision the skipper-server.
+
WARNING: Only set 'Skip SSL Validation' to true if you're running on a Cloud Foundry instance using self-signed
certs (e.g. in development). Do not use for production.

. Configure and run the Data Flow Server
+
One of the most important configurations is providing providing credentials to the Cloud Foundry instance so that the server can spawn applications itself.
Any Spring Boot compatible configuration mechanism can be used (passing program arguments, editing configuration files before building the application, using
link:https://github.com/spring-cloud/spring-cloud-config[Spring Cloud Config], using environment variables, etc.), although some may prove more practicable than others depending how you typically deploy applications to Cloud Foundry.

In this next section we will show how to deploy Data Flow using environment variables or a Cloud Foundry manifest.
However, there are some general configuration details you should be aware of in either approach.

[[getting-started-cloudfoundry-general-configuration]]
=== General Configuration

NOTE: You must use a unique name for your app; an app with the same name in the same organization will cause your
deployment to fail

NOTE: The recommended minimal memory setting for the server is 2G. Also, to push apps to PCF and obtain
application property metadata, the server downloads applications to Maven repository hosted on the local disk.  While
you can specify up to 2G as a typical maximum value for disk space on a PCF installation, this can be increased to
10G.  Read the xref:getting-started-maximum-disk-quota-configuration[maximum disk quota] section for information on
how to configure this PCF property.  Also, the Data Flow server itself implements a Last Recently Used algorithm to
free disk space when it falls below a low water mark value.

NOTE: If you are pushing to a space with multiple users, for example on PWS, there may already be a route taken for the
applicaiton name you have chosen. You can use the options `--random-route` to avoid this when pushing the server application.

NOTE: By default, the https://github.com/spring-cloud/spring-cloud-dataflow/tree/master/spring-cloud-dataflow-registry[application registry] in Spring Cloud Data Flow's Cloud Foundry server is empty. It is intentionally designed to allow users to have the flexibility of http://docs.spring.io/spring-cloud-dataflow/docs/{scdf-core-version}/reference/htmlsingle/#spring-cloud-dataflow-register-stream-apps[choosing and registering] applications, as they find appropriate for the given use-case requirement. Depending on the message-binder of choice, users can register between http://repo.spring.io/libs-snapshot/org/springframework/cloud/stream/app/[RabbitMQ or Apache Kafka] based maven artifacts.

NOTE: If you need to configure multiple Maven repositories, a proxy, or authorization for a private repository, see link:http://docs.spring.io/spring-cloud-dataflow/docs/{scdf-core-version}/reference/htmlsingle/#getting-started-maven-configuration[Maven Configuration].

[[getting-started-cloudfoundry-deploying-using-env-vars]]
=== Deploying using environment variables

The following configuration is for Pivotal Web Services. You need to fill in \{org}, \{space},
\{email} and \{password} before running these commands.

```
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_URL https://api.run.pivotal.io
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_ORG {org}
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SPACE {space}
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_DOMAIN cfapps.io
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_SERVICES rabbit
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_SERVICES my_mysql
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_USERNAME {email}
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_PASSWORD {password}
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SKIP_SSL_VALIDATION false
```

NOTE: If you are going to use `Skipper` to deploy Streams, deploy Skipper first and then configure the uri location of where the Skipper server is running and set the server to be in 'skipper mode

----
cf set-env dataflow-server SPRING_CLOUD_SKIPPER_CLIENT_SERVER_URI https://<skipper-host-name>/api
cf set-env dataflow-server SPRING_CLOUD_DATAFLOW_FEATURES_SKIPPER_ENABLED true
----

The Spring Cloud Data Flow server does not have _any_ default remote maven repository configured.
This is intentionally designed to provide the flexibility the users, so they can override and point to a remote repository of their choice.
The out-of-the-box applications that are supported by Spring Cloud Data Flow are available in Spring's repository, so if you want to use them, set it as the remote repository as listed below.

```
cf set-env dataflow-server SPRING_APPLICATION_JSON '{"maven": { "remote-repositories": { "repo1": { "url": "https://repo.spring.io/libs-release" } } } }'
```
where `repo1` is the alias name for the remote repository.

WARNING: Only set 'Skip SSL Validation' to true if you're running on a Cloud Foundry instance using self-signed
certs (e.g. in development). Do not use for production.

NOTE: If you are deploying in an environment that requires you to sign on using the Pivotal Single Sign-On Service,
refer to the section <<getting-started-security-cloud-foundry>> for information on how to configure the server.

You can issue now `cf push` command and reference the Data Flow server .jar.

[subs=attributes]
```
cf push dataflow-server -b java_buildpack -m 2G -k 2G --no-start -p spring-cloud-dataflow-server-cloudfoundry-{project-version}.jar
cf bind-service dataflow-server redis
cf bind-service dataflow-server my_mysql
```

[[getting-started-cloudfoundry-deploying-using-manifest]]
=== Deploying using a Manifest

As an alternative to setting environment variables via `cf set-env` command, you can curate all the relevant env-var's
in `manifest.yml` file and use `cf push` command to provision the server.

Following is a sample template to provision the server on PCFDev.

[source,yml]
----
---
applications:
- name: data-flow-server
  host: data-flow-server
  memory: 2G
  disk_quota: 2G
  instances: 1
  path: {PATH TO SERVER UBER-JAR}
  env:
    SPRING_APPLICATION_NAME: data-flow-server
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_URL: https://api.local.pcfdev.io
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_ORG: pcfdev-org
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SPACE: pcfdev-space
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_DOMAIN: local.pcfdev.io
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_USERNAME: admin
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_PASSWORD: admin
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_SERVICES: rabbit
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_SERVICES: mysql
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SKIP_SSL_VALIDATION: true
    SPRING_APPLICATION_JSON {"maven": { "remote-repositories": { "repo1": { "url": "https://repo.spring.io/libs-release"} } } }
services:
- mysql
----

NOTE: If you are going to use `Skipper` to deploy Streams, deploy Skipper first and then configure the uri location of where the Skipper server is running and set the feature toggle to use skipper.

[source,yml]
----
applications:
  env:
    SPRING_CLOUD_SKIPPER_CLIENT_SERVER_URI: https://<skipper-host-name>/api
    SPRING_CLOUD_DATAFLOW_FEATURES_SKIPPER_ENABLED: true
----

Once you're ready with the relevant properties in this file, you can issue `cf push` command from the directory where
this file is stored.

[[getting-started-cloudfoundry-on-local]]
== Local Installation

To run the server application locally, on your laptop or desktop, and target your Cloud Foundry installation, configure the Data Flow server by setting the following environment variables.

```
export SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_URL=https://api.run.pivotal.io
export SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_ORG={org}
export SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SPACE={space}
export SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_DOMAIN=cfapps.io
export SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_USERNAME={email}
export SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_PASSWORD={password}
export SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SKIP_SSL_VALIDATION=false

export SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_SERVICES=rabbit
# The following is for letting task apps write to their db.
# Note however that when the *server* is running locally, it can't access that db
# task related commands that show executions won't work then
export SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_SERVICES=my_mysql
```

You need to fill in \{org}, \{space}, \{email} and \{password} before running these commands.

WARNING: Only set 'Skip SSL Validation' to true if you're running on a Cloud Foundry instance using self-signed
certs (e.g. in development). Do not use for production.

NOTE: If you are going to use `Skipper` to deploy Streams, deploy Skipper first and then configure the uri location of where the Skipper server is running and enable the Skipper feature toggle..

NOTE: Since Skipper is a Spring Boot application, you can also pass the configuration properties as command line options instead of environment variables.  `SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_STREAM_SERVICES` becomes `--spring.cloud.deployer.cloudfoundry.stream.services`.

----
export SKIPPER_CLIENT_HOST https://<skipper-host-name>/api
export SPRING_CLOUD_DATAFLOW_FEATURES_SKIPPER_ENABLED true
----

Now we are ready to start the server application:

[subs=attributes]
```
java -jar spring-cloud-dataflow-server-cloudfoundry-{project-version}.jar
```

TIP: Of course, all other parameterization options that were available when running the server _on_ Cloud Foundry are
still available. This is particularly true for xref:configuring-defaults[configuring defaults] for applications. Just
substitute `cf set-env` syntax with `export`.

[[getting-started-data-flow-shell]]
== Data Flow Shell
Launching the Data Flow shell requires the appropriate data flow server mode to be specified.
To start the Data Flow Shell for the Data Flow server running in `classic` mode:

[source,bash,subs=attributes]
----
$ java -jar spring-cloud-dataflow-shell-{dataflow-project-version}.jar
----

[[getting-started-deploying-streams]]
== Deploying Streams

. Import Apps
+
By default, the application registry will be empty. If you would like to register all out-of-the-box stream applications
built with the RabbitMQ binder in bulk, you can with the following command. For more details, review how to
xref:spring-cloud-dataflow-register-apps[register applications].
+
```
dataflow:>app import --uri http://bit.ly/Celsius-SR1-stream-applications-rabbit-maven
```
+
There are two options for deploying Streams. The "traditional" way that Data Flow has always used and a new way that delegates to the Skipper server. Deploying using Skipper will enable you to update and rollback the streams while the traditional way will not.
+
. Create Streams without skipper
+
Create a simple stream with an HTTP source and a log sink.
+
[source]
----
dataflow:> stream create --name httptest --definition "http | log" --deploy
----
+
NOTE: You will need to wait a little while until the apps are actually deployed successfully
before posting data.  Tail the log file for each application to verify
the application has started.
+
Now post some data. The URL will be unique to your deployment, the following is just an example
+
[source]
----
dataflow:> http post --target http://dataflow-AxwwAhK-httptest-http.cfapps.io --data "hello world"
----
Look to see if `hello world` ended up in log files for the `log` application.
+
. Create Streams with Skipper
This section assumes you have deployed Skipper and configured the Data Flow server's `SPRING_CLOUD_SKIPPER_CLIENT_SERVER_URI` property to reference the Skipper server.
+
[source]
----
dataflow:> stream create --name httptest --definition "http | log"
dataflow:> stream deploy --name httptest --platformName pws
----
+
Look to see if `hello world` ended up in log files for the `log` application.
+
[NOTE]
====
Skipper includes the concept of link:https://docs.spring.io/spring-cloud-skipper/docs/current/reference/htmlsingle/#platforms[platforms],
so it is important to define the "accounts" based on the project preferences. In the above YAML file, the accounts map
to `pws` as the platform. This can be modified, and of course, you can have any number of platform definitions.
More details are in Spring Cloud Skipper reference guide.
====
+
You can read more about the general features of using Skipper to deploy streams in the section <<spring-cloud-dataflow-stream-lifecycle-skipper>> and how to upgrade a streams in the section <<spring-cloud-dataflow-stream-lifecycle-skipper-update>>.


[[streams-using-skipper]]
== Deploying Streams using Skipper

We will proceed with the assumption that Spring Cloud Data Flow, Spring Cloud Skipper, RDBMS, and desired messaging
middleware is up and running in PWS.

[source,console,options=nowrap]
----
$ cf apps                                                                                                           ✭
Getting apps in org ORG / space SPACE as email@pivotal.io...
OK

name                         requested state   instances   memory   disk   urls
skipper-server               started           1/1         1G       1G     skipper-server.cfapps.io
dataflow-server              started           1/1         1G       1G     dataflow-server.cfapps.io
----

Start the Data Flow Shell for the Data Flow server running in `skipper` mode:

[source,bash,subs=attributes]
----
$ java -jar spring-cloud-dataflow-shell-{dataflow-project-version}.jar --dataflow.mode=skipper
----
If the Data Flow Server and shell are not running on the same host, point the shell to the Data Flow server URL:
```
server-unknown:>dataflow config server http://dataflow-server.cfapps.io
Successfully targeted http://dataflow-server.cfapps.io
dataflow:>
```
Alternatively, pass in the command line option `--dataflow.uri`.  The shell's command line option `--help` shows what options are available.

Verify the available platforms in Skipper.

[source,console,options=nowrap]
----
dataflow:>stream platform-list
╔═══════╤════════════╤═════════════════════════════════════════════════════════════════════════════════════╗
║ Name  │    Type    │                                                 Description                         ║
╠═══════╪════════════╪═════════════════════════════════════════════════════════════════════════════════════╣
║pws    │cloudfoundry│org = [scdf-ci], space = [space-sabby], url = [https://api.run.pivotal.io]           ║
╚═══════╧════════════╧═════════════════════════════════════════════════════════════════════════════════════╝
----

Let's start with deploying a stream with the `time-source` pointing to 1.2.0.RELEASE and `log-sink` pointing
to 1.1.0.RELEASE. The goal is to rolling upgrade the `log-sink` application to 1.2.0.RELEASE.

[source,console,options=nowrap]
----
dataflow:>app register --name time --type source --uri maven://org.springframework.cloud.stream.app:time-source-rabbit:1.2.0.RELEASE --force
Successfully registered application 'source:time'

dataflow:>app register --name log --type sink --uri maven://org.springframework.cloud.stream.app:log-sink-rabbit:1.1.0.RELEASE --force
Successfully registered application 'sink:log'

dataflow:>app info source:time
Information about source application 'time':
Resource URI: maven://org.springframework.cloud.stream.app:time-source-rabbit:1.2.0.RELEASE

dataflow:>app info sink:log
Information about sink application 'log':
Resource URI: maven://org.springframework.cloud.stream.app:log-sink-rabbit:1.1.0.RELEASE
----

. Create stream.
+
Use a unique name, one that might not be taken by another application on PCF/PWS.
```
dataflow:>stream create ticker-314 --definition "time | log"
Created new stream 'ticker-314'
```
+

. Deploy stream.

+
```
dataflow:>stream deploy ticker-314 --platformName pws
Deployment request has been sent for stream 'ticker-314'
```
+

[NOTE]
====
While deploying the stream, we are supplying `--platformName` and that indicates the platform repository (i.e., `pws`) to
use when deploying the stream applications via Skipper.
====

. List apps.

+
[source,console,options=nowrap]
----
$ cf apps                                                                                                                                                                                                                                         [1h] ✭
Getting apps in org ORG / space SPACE as email@pivotal.io...

name                         requested state   instances   memory   disk   urls
ticker-314-log-v1            started           1/1         1G       1G     ticker-314-log-v1.cfapps.io
ticker-314-time-v1           started           1/1         1G       1G     ticker-314-time-v1.cfapps.io
skipper-server               started           1/1         1G       1G     skipper-server.cfapps.io
dataflow-server              started           1/1         1G       1G     dataflow-server.cfapps.io
----
+

. Verify logs.

+
[source,console,options=nowrap]
----
$ cf logs ticker-314-log-v1
...
...
2017-11-20T15:39:43.76-0800 [APP/PROC/WEB/0] OUT 2017-11-20 23:39:43.761  INFO 12 --- [ ticker-314.time.ticker-314-1] log-sink                                 : 11/20/17 23:39:43
2017-11-20T15:39:44.75-0800 [APP/PROC/WEB/0] OUT 2017-11-20 23:39:44.757  INFO 12 --- [ ticker-314.time.ticker-314-1] log-sink                                 : 11/20/17 23:39:44
2017-11-20T15:39:45.75-0800 [APP/PROC/WEB/0] OUT 2017-11-20 23:39:45.757  INFO 12 --- [ ticker-314.time.ticker-314-1] log-sink                                 : 11/20/17 23:39:45
----
+

. Verify the stream history.

+
[source,console,options=nowrap]
----
dataflow:>stream history --name ticker-314
╔═══════╤════════════════════════════╤════════╤════════════╤═══════════════╤════════════════╗
║Version│        Last updated        │ Status │Package Name│Package Version│  Description   ║
╠═══════╪════════════════════════════╪════════╪════════════╪═══════════════╪════════════════╣
║1      │Mon Nov 20 15:34:37 PST 2017│DEPLOYED│ticker-314  │1.0.0          │Install complete║
╚═══════╧════════════════════════════╧════════╧════════════╧═══════════════╧════════════════╝
----
+

. Verify the package manifest in Skipper. The `log-sink` should be at 1.1.0.RELEASE.

+
[source,yml,options=nowrap]
----
dataflow:>stream manifest --name ticker-314

---
# Source: log.yml
apiVersion: skipper.spring.io/v1
kind: SpringCloudDeployerApplication
metadata:
  name: log
spec:
  resource: maven://org.springframework.cloud.stream.app:log-sink-rabbit
  version: 1.1.0.RELEASE
  applicationProperties:
    spring.cloud.dataflow.stream.app.label: log
    spring.cloud.stream.metrics.properties: spring.application.name,spring.application.index,spring.cloud.application.*,spring.cloud.dataflow.*
    spring.cloud.stream.bindings.applicationMetrics.destination: metrics
    spring.cloud.dataflow.stream.name: ticker-314
    spring.metrics.export.triggers.application.includes: integration**
    spring.cloud.stream.metrics.key: ticker-314.log.${spring.cloud.application.guid}
    spring.cloud.stream.bindings.input.group: ticker-314
    spring.cloud.dataflow.stream.app.type: sink
    spring.cloud.stream.bindings.input.destination: ticker-314.time
  deploymentProperties:
    spring.cloud.deployer.indexed: true
    spring.cloud.deployer.group: ticker-314

---
# Source: time.yml
apiVersion: skipper.spring.io/v1
kind: SpringCloudDeployerApplication
metadata:
  name: time
spec:
  resource: maven://org.springframework.cloud.stream.app:time-source-rabbit
  version: 1.2.0.RELEASE
  applicationProperties:
    spring.cloud.dataflow.stream.app.label: time
    spring.cloud.stream.metrics.properties: spring.application.name,spring.application.index,spring.cloud.application.*,spring.cloud.dataflow.*
    spring.cloud.stream.bindings.applicationMetrics.destination: metrics
    spring.cloud.dataflow.stream.name: ticker-314
    spring.metrics.export.triggers.application.includes: integration**
    spring.cloud.stream.metrics.key: ticker-314.time.${spring.cloud.application.guid}
    spring.cloud.stream.bindings.output.producer.requiredGroups: ticker-314
    spring.cloud.stream.bindings.output.destination: ticker-314.time
    spring.cloud.dataflow.stream.app.type: source
  deploymentProperties:
    spring.cloud.deployer.group: ticker-314
----

. Let's update `log-sink` from 1.1.0.RELEASE to 1.2.0.RELEASE.  First we need to register the version 1.2.0.RELEASE.
+
[source,console,options=nowrap]
----
dataflow:>app register --name log --type sink --uri maven://org.springframework.cloud.stream.app:log-sink-rabbit:1.1.0.RELEASE --force
Successfully registered application 'sink:log'
----
+
If you execute the `app list` command for the log sink, you will now see that two versions are registered.

+
[source,console,options=nowrap]
----
dataflow:>app list --id sink:log
╔══════╤═════════╤═════════════════════╤════╗
║source│processor│        sink         │task║
╠══════╪═════════╪═════════════════════╪════╣
║      │         │> log-1.1.0.RELEASE <│    ║
║      │         │log-1.2.0.RELEASE    │    ║
╚══════╧═════════╧═════════════════════╧════╝
----
+
The greater than and less than signs, `> log-1.1.0.RELEASE <` indicate that this is the default version that will be used when matching `log` in the DSL for a stream definition.
You can change the default version using the command `app default`
+
[source,console,options=nowrap]
----
dataflow:>stream update --name ticker-314 --properties version.log=1.2.0.RELEASE
Update request has been sent for stream 'ticker-314'
----
+

. List apps.

+
[source,console,options=nowrap]
----
± cf apps                                                                                                                                                                                                                                         [1h] ✭
Getting apps in org ORG / space SPACE as email@pivotal.io...

Getting apps in org scdf-ci / space space-sabby as sanandan@pivotal.io...
OK

name                         requested state   instances   memory   disk   urls
ticker-314-log-v2            started           1/1         1G       1G     ticker-314-log-v2.cfapps.io
ticker-314-log-v1            stopped           0/1         1G       1G
ticker-314-time-v1           started           1/1         1G       1G     ticker-314-time-v1.cfapps.io
skipper-server               started           1/1         1G       1G     skipper-server.cfapps.io
dataflow-server              started           1/1         1G       1G     dataflow-server.cfapps.io
----
+

[NOTE]
====
Notice that there are two versions of the `log-sink` applications. The `ticker-314-log-v1` application instance is going down
(route already removed) and the newly spawned `ticker-314-log-v2` application is bootstrapping. The version number is incremented and
the version-number (`v2`) is included in the new application name.
====

. Once the new application is up and running, let's verify the logs.

+
[source,console,options=nowrap]
----
$ cf logs ticker-314-log-v2
...
...
2017-11-20T18:38:35.00-0800 [APP/PROC/WEB/0] OUT 2017-11-21 02:38:35.003  INFO 18 --- [ticker-314.time.ticker-314-1] ticker-314-log-v2                              : 11/21/17 02:38:34
2017-11-20T18:38:36.00-0800 [APP/PROC/WEB/0] OUT 2017-11-21 02:38:36.004  INFO 18 --- [ticker-314.time.ticker-314-1] ticker-314-log-v2                              : 11/21/17 02:38:35
2017-11-20T18:38:37.00-0800 [APP/PROC/WEB/0] OUT 2017-11-21 02:38:37.005  INFO 18 --- [ticker-314.time.ticker-314-1] ticker-314-log-v2                              : 11/21/17 02:38:36
----
+

. Let's look at the updated package manifest persisted in Skipper. We should now be seeing `log-sink` at 1.2.0.RELEASE.

+
[source,yml,options=nowrap]
----
skipper:>stream manifest --name ticker-314
---
# Source: log.yml
apiVersion: skipper.spring.io/v1
kind: SpringCloudDeployerApplication
metadata:
  name: log
spec:
  resource: maven://org.springframework.cloud.stream.app:log-sink-rabbit
  version: 1.2.0.RELEASE
  applicationProperties:
    spring.cloud.dataflow.stream.app.label: log
    spring.cloud.stream.metrics.properties: spring.application.name,spring.application.index,spring.cloud.application.*,spring.cloud.dataflow.*
    spring.cloud.stream.bindings.applicationMetrics.destination: metrics
    spring.cloud.dataflow.stream.name: ticker-314
    spring.metrics.export.triggers.application.includes: integration**
    spring.cloud.stream.metrics.key: ticker-314.log.${spring.cloud.application.guid}
    spring.cloud.stream.bindings.input.group: ticker-314
    spring.cloud.dataflow.stream.app.type: sink
    spring.cloud.stream.bindings.input.destination: ticker-314.time
  deploymentProperties:
    spring.cloud.deployer.indexed: true
    spring.cloud.deployer.group: ticker-314
    spring.cloud.deployer.count: 1

---
# Source: time.yml
apiVersion: skipper.spring.io/v1
kind: SpringCloudDeployerApplication
metadata:
  name: time
spec:
  resource: maven://org.springframework.cloud.stream.app:time-source-rabbit
  version: 1.2.0.RELEASE
  applicationProperties:
    spring.cloud.dataflow.stream.app.label: time
    spring.cloud.stream.metrics.properties: spring.application.name,spring.application.index,spring.cloud.application.*,spring.cloud.dataflow.*
    spring.cloud.stream.bindings.applicationMetrics.destination: metrics
    spring.cloud.dataflow.stream.name: ticker-314
    spring.metrics.export.triggers.application.includes: integration**
    spring.cloud.stream.metrics.key: ticker-314.time.${spring.cloud.application.guid}
    spring.cloud.stream.bindings.output.producer.requiredGroups: ticker-314
    spring.cloud.stream.bindings.output.destination: ticker-314.time
    spring.cloud.dataflow.stream.app.type: source
  deploymentProperties:
    spring.cloud.deployer.group: ticker-314
----
+

. Verify stream history for the latest updates.

+
[source,console,options=nowrap]
----
dataflow:>stream history --name ticker-314
╔═══════╤════════════════════════════╤════════╤════════════╤═══════════════╤════════════════╗
║Version│        Last updated        │ Status │Package Name│Package Version│  Description   ║
╠═══════╪════════════════════════════╪════════╪════════════╪═══════════════╪════════════════╣
║2      │Mon Nov 20 15:39:37 PST 2017│DEPLOYED│ticker-314  │1.0.0          │Upgrade complete║
║1      │Mon Nov 20 15:34:37 PST 2017│DELETED │ticker-314  │1.0.0          │Delete complete ║
╚═══════╧════════════════════════════╧════════╧════════════╧═══════════════╧════════════════╝
----
+

. Rolling-back to the previous version is just a command away.

+
[source,console,options=nowrap]
----
dataflow:>stream rollback --name ticker-314
Rollback request has been sent for the stream 'ticker-314'

...
...

dataflow:>stream history --name ticker-314
╔═══════╤════════════════════════════╤════════╤════════════╤═══════════════╤════════════════╗
║Version│        Last updated        │ Status │Package Name│Package Version│  Description   ║
╠═══════╪════════════════════════════╪════════╪════════════╪═══════════════╪════════════════╣
║3      │Mon Nov 20 15:41:37 PST 2017│DEPLOYED│ticker-314  │1.0.0          │Upgrade complete║
║2      │Mon Nov 20 15:39:37 PST 2017│DELETED │ticker-314  │1.0.0          │Delete complete ║
║1      │Mon Nov 20 15:34:37 PST 2017│DELETED │ticker-314  │1.0.0          │Delete complete ║
╚═══════╧════════════════════════════╧════════╧════════════╧═══════════════╧════════════════╝
----


[[getting-started-deploying-tasks]]
== Deploying Tasks

To run a simple task application, you can register all the out-of-the-box task applications with the following command.

```
dataflow:>app import --uri http://bit.ly/Clark-GA-task-applications-maven

```

Now create a simple link:https://docs.spring.io/spring-cloud-task-app-starters/docs/Clark.RELEASE/reference/html/_timestamp_task.html[timestamp] task.

```
dataflow:>task create mytask --definition "timestamp --format='yyyy'"
```

Tail the logs, e.g. `cf logs mytask` and then launch the task in the UI or in the Data Flow Shell

```
dataflow:>task launch mytask
```

You will see the year `2017` printed in the logs. The execution status of the task is stored
in the database and you can retrieve information about the task execution using the shell commands
`task execution list` and `task execution status --id <ID_OF_TASK>` or though the Data Flow UI.

NOTE: The current underlying PCF task capabilities are considered experimental for PCF version
versions less than 1.9.  See http://docs.spring.io/spring-cloud-dataflow/docs/{scdf-core-version}/reference/htmlsingle/#enable-disable-specific-features[Feature Togglers]
for how to disable task support in Data Flow.

