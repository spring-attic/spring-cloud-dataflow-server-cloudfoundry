[[getting-started]]
= Getting Started

[[getting-started-requirements]]
== System Requirements

The Spring Cloud Data Flow server deploys task tasks (short-lived applications) and via Skipper deploys streams (long-lived applications) to Cloud Foundry.
The server is a lightweight Spring Boot application. It can run on Cloud Foundry or your laptop, but it is more common to run the server in Cloud Foundry.

Spring Cloud Data Flow requires a few data services to perform streaming, task/batch processing, and analytics.
You have two options when you provision Spring Cloud Data Flow and related services on Cloud Foundry:

* The simplest (and automated) method is to use the link:https://network.pivotal.io/products/p-dataflow[Spring Cloud Data Flow for PCF] tile.
This is an opinionated tile for Pivotal Cloud Foundry.
It automatically provisions the server and the required data services, thus simplifying the overall getting-started experience. You can read more about the installation link:http://docs.pivotal.io/scdf/[here].
* Alternatively, you can provision all the components manually. The following section goes into the specifics of how to do so.

=== Provision a Redis Service Instance on Cloud Foundry
A Redis instance is required for analytics apps and is typically bound to such apps when you create an analytics stream by using the <<getting-started.adoc#getting-started-service-binding-at-application-level,per-app-binding>> feature.

You can use `cf marketplace` to discover which plans are available to you, depending on the details of your Cloud Foundry setup.
For example, you can use link:https://run.pivotal.io/[Pivotal Web Services], as the following example shows:

====
[source]
----
cf create-service rediscloud 30mb redis
----
====

=== Provision a Rabbit Service Instance on Cloud Foundry
RabbitMQ is used as a messaging middleware between streaming apps and is bound to each deployed streaming app.
Apache Kafka is the other option.
We can use the `SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_DEPLOYMENT_SERVICES` setting in `Skipper Server` configuration which automatically binds RabbitMQ to the deployed streaming applications.

You can use `cf marketplace` to discover which plans are available to you, depending on the details of your Cloud Foundry setup.
For example, you can use link:https://run.pivotal.io/[Pivotal Web Services]:

====
[source]
----
cf create-service cloudamqp lemur rabbit
----
====

=== Provision a MySQL Service Instance on Cloud Foundry
An RDBMS is used to persist Data Flow state, such as stream and task definitions, deployments, and executions.

You can use `cf marketplace` to discover which plans are available to you, depending on the details of your Cloud Foundry setup.
For example, you can use link:https://run.pivotal.io/[Pivotal Web Services]:

====
[source]
----
cf create-service cleardb spark my_mysql
----
====

[[getting-started-cloudfoundry]]
== Cloud Foundry Installation
Starting with 2.0.x, the Data Flow Server requires a `Skipper` server for managing the Streams lifecycle.

. Download the Data Flow server and shell applications, as the following example shows:
+
====
[source,yaml,subs=attributes]
----
wget http://repo.spring.io/{version-type-lowercase}/org/springframework/cloud/spring-cloud-dataflow-server-cloudfoundry/{project-version}/spring-cloud-dataflow-server-cloudfoundry-{project-version}.jar

wget http://repo.spring.io/{dataflow-version-type-lowercase}/org/springframework/cloud/spring-cloud-dataflow-shell/{dataflow-project-version}/spring-cloud-dataflow-shell-{dataflow-project-version}.jar
----
====
. Download http://cloud.spring.io/spring-cloud-skipper/[Skipper] to which Data Flow delegate Streams lifecycle operations such as deployment, upgrading and rolling back.
The following example shows how to do so:
+
====
[source,yaml,options=nowrap,subs=attributes]
----
wget http://repo.spring.io/{skipper-version-type-lowercase}/org/springframework/cloud/spring-cloud-skipper-server/{skipper-version}/spring-cloud-skipper-server-{skipper-version}.jar
----
====
+
Push Skipper to Cloud Foundry.  The following example shows a manifest for Skipper.
+
====
[source,yaml,options=nowrap]
----
---
applications:
- name: skipper-server
  host: skipper-server
  memory: 1G
  disk_quota: 1G
  instances: 1
  timeout: 180
  buildpack: java_buildpack
  path: <PATH TO THE DOWNLOADED SKIPPER SERVER UBER-JAR>
  env:
    SPRING_APPLICATION_NAME: skipper-server
    SPRING_CLOUD_SKIPPER_SERVER_ENABLE_LOCAL_PLATFORM: false
    SPRING_CLOUD_SKIPPER_SERVER_STRATEGIES_HEALTHCHECK_TIMEOUTINMILLIS: 300000
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_CONNECTION_URL: https://api.run.pivotal.io
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_CONNECTION_ORG: {org}
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_CONNECTION_SPACE: {space}
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_DEPLOYMENT_DOMAIN: cfapps.io
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_CONNECTION_USERNAME: {email}
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_CONNECTION_PASSWORD: {password}
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_CONNECTION_SKIP_SSL_VALIDATION: false
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_DEPLOYMENT_DELETE_ROUTES: false
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_DEPLOYMENT_SERVICES: {middlewareServiceName}
    SPRING_CLOUD_SKIPPER_SERVER_PLATFORM_CLOUDFOUNDRY_ACCOUNTS[pws]_DEPLOYMENT_STREAM_ENABLE_RANDOM_APP_NAME_PREFIX: false
services:
- {services}
----
====
+
You need to fill in `\{org}`, `\{space}`, `\{email}`,  `\{password}`, `{middlewareServiceName}` (e.g. rabbit or kafka) and {services} (such as mysql) before running these commands.
Once you have the desired config values in `manifest.yml`, you can run the `cf push` command to provision the skipper-server.
+
WARNING: Only set 'Skip SSL Validation' to `true` if you run on a Cloud Foundry instance by using self-signed
certificates (for example, in development). Do not use self-signed certificates for production.
+
. Configure and run the Data Flow Server
+
One of the most important configuration details is providing credentials to the Cloud Foundry instance so that the server can itself spawn applications.
You can use any Spring Boot-compatible configuration mechanism (passing program arguments, editing configuration files before building the application, using link:https://github.com/spring-cloud/spring-cloud-config[Spring Cloud Config], using environment variables, and others), although some may prove more practicable than others, depending on how you typically deploy applications to Cloud Foundry.

In later sections, we show how to deploy Data Flow by using <<getting-started-cloudfoundry-deploying-using-env-vars,environment variables>> or a <<getting-started-cloudfoundry-deploying-using-manifest,Cloud Foundry manifest>>.
However, there are some general configuration details you should be aware of in either approach.

[[getting-started-cloudfoundry-general-configuration]]
=== General Configuration

NOTE: You must use a unique name for your app. An application with the same name in the same organization causes your deployment to fail.

NOTE: The recommended minimum memory setting for the server is 2G. Also, to push apps to PCF and obtain application property metadata, the server downloads applications to a Maven repository hosted on the local disk.
While you can specify up to 2G as a typical maximum value for disk space on a PCF installation, you can increase this to 10G.
Read the xref:getting-started-maximum-disk-quota-configuration[maximum disk quota] section for information on how to configure this PCF property.
Also, the Data Flow server itself implements a Last-Recently-Used algorithm to free disk space when it falls below a low-water-mark value.

NOTE: If you are pushing to a space with multiple users (for example, on PWS), the route you chose for your application name may already be taken.
You can use the `--random-route` option to avoid this when you push the server application.

NOTE: By default, the https://github.com/spring-cloud/spring-cloud-dataflow/tree/master/spring-cloud-dataflow-registry[application registry] in Spring Cloud Data Flow's Cloud Foundry server is empty.
It is intentionally designed to let you have the flexibility of http://docs.spring.io/spring-cloud-dataflow/docs/{scdf-core-version}/reference/htmlsingle/#spring-cloud-dataflow-register-stream-apps[choosing and registering] applications as you find appropriate for the given use-case requirement. Depending on the message-binder you choose, you can register between http://repo.spring.io/libs-snapshot/org/springframework/cloud/stream/app/[RabbitMQ- or Apache Kafka-based] Maven artifacts.

NOTE: If you need to configure multiple Maven repositories, a proxy, or authorization for a private repository, see link:http://docs.spring.io/spring-cloud-dataflow/docs/{scdf-core-version}/reference/htmlsingle/#getting-started-maven-configuration[Maven Configuration].

[[getting-started-cloudfoundry-deploying-using-env-vars]]
=== Deploying by Using Environment Variables

The following configuration is for Pivotal Web Services. You need to fill in `\{org}`, `\{space}`, `\{email}` and `\{password}` before running these commands.

====
[source]
----
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_URL https://api.run.pivotal.io
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_ORG {org}
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SPACE {space}
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_DOMAIN cfapps.io
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_SERVICES my_mysql
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_USERNAME {email}
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_PASSWORD {password}
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SKIP_SSL_VALIDATION false
cf set-env dataflow-server SPRING_CLOUD_SKIPPER_CLIENT_SERVER_URI https://<skipper-host-name>/api
----
====

[NOTE]
=====
Deploy Skipper first and then configure the URI location where the Skipper server runs.
=====

The Spring Cloud Data Flow server does not have any default remote maven repository configured.
This is intentionally designed to provide the flexibility, so you can override and point to a remote repository of your choice.
The out-of-the-box applications that are supported by Spring Cloud Data Flow are available in Spring's repository. If you want to use them, set it as the remote repository, as the following example shows:

====
[source]
----
cf set-env dataflow-server SPRING_APPLICATION_JSON '{"maven": { "remote-repositories": { "repo1": { "url": "https://repo.spring.io/libs-release" } } } }'
----
where `repo1` is the alias name for the remote repository
====

WARNING: Only set 'Skip SSL Validation' to true if you run on a Cloud Foundry instance using self-signed certificates (for example, in development).
Do not use self-signed certificates for production.

NOTE: If you are deploying in an environment that requires you to sign on using the Pivotal Single Sign-On Service, see <<getting-started-security-cloud-foundry>> for information on how to configure the server.

You can now issue a `cf push` command and reference the Data Flow server .jar file, as the following example shows:

====
[source, subs=attributes]
----
cf push dataflow-server -b java_buildpack -m 2G -k 2G --no-start -p spring-cloud-dataflow-server-cloudfoundry-{project-version}.jar
cf bind-service dataflow-server redis
cf bind-service dataflow-server my_mysql
----
====

[[getting-started-cloudfoundry-deploying-using-manifest]]
=== Deploying by Using a Manifest

As an alternative to setting environment variables with the `cf set-env` command, you can curate all the relevant env-var's in a `manifest.yml` file and use the `cf push` command to provision the server.

The following example template provisions the server on PCFDev:

====
[source,yml]
----
---
applications:
- name: data-flow-server
  host: data-flow-server
  memory: 2G
  disk_quota: 2G
  instances: 1
  path: {PATH TO SERVER UBER-JAR}
  env:
    SPRING_APPLICATION_NAME: data-flow-server
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_URL: https://api.local.pcfdev.io
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_ORG: pcfdev-org
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SPACE: pcfdev-space
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_DOMAIN: local.pcfdev.io
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_USERNAME: admin
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_PASSWORD: admin
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_SERVICES: mysql
    SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SKIP_SSL_VALIDATION: true
    SPRING_CLOUD_SKIPPER_CLIENT_SERVER_URI: https://<skipper-host-name>/api
    SPRING_APPLICATION_JSON: '{"maven": { "remote-repositories": { "repo1": { "url": "https://repo.spring.io/libs-release"} } } }'
services:
- mysql
----
====

[NOTE]
=====
Deploy Skipper first and then configure the URI location where the Skipper server runs.
=====

Once you are ready with the relevant properties in this file, you can issue a `cf push` command from the directory where this file is stored.

[[getting-started-cloudfoundry-on-local]]
== Local Installation

To run the server application locally (on your laptop or desktop) and target your Cloud Foundry installation, configure the Data Flow server by setting the following environment variables.

====
[source]
----
export SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_URL=https://api.run.pivotal.io
export SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_ORG={org}
export SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SPACE={space}
export SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_DOMAIN=cfapps.io
export SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_USERNAME={email}
export SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_PASSWORD={password}
export SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SKIP_SSL_VALIDATION=false

# The following is for letting task apps write to their db.
# Note however that when the *server* is running locally, it can't access that db
# task related commands that show executions won't work then
export SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_SERVICES=my_mysql
export SKIPPER_CLIENT_HOST https://<skipper-host-name>/api
----
====

You need to fill in `\{org}`, `\{space}`, `\{email}` and `\{password}` before running these commands.

WARNING: Only set 'Skip SSL Validation' to true if you run on a Cloud Foundry instance using self-signed certificates (for example, in development).
Do not use self-signed certificates for production.

[NOTE]
=====
Deploy Skipper first and then configure the URI location of where the Skipper server is running.
=====

Now we are ready to start the server application, as follows:

====
[source, subs=attributes]
----
java -jar spring-cloud-dataflow-server-cloudfoundry-{project-version}.jar
----
====

TIP: All other parameterization options that were available when running the server on Cloud Foundry are still available.
This is particularly true for xref:configuring-defaults[configuring defaults] for applications. To use them, substitute `cf set-env` syntax with `export`.

[[getting-started-data-flow-shell]]
== Data Flow Shell
The following example shows how to start the Data Flow Shell:

====
[source,bash,subs=attributes]
----
$ java -jar spring-cloud-dataflow-shell-{dataflow-project-version}.jar
----
====

[[getting-started-deploying-streams]]
== Deploying Streams

By default, the application registry is empty.
If you would like to register all out-of-the-box stream applications built with the RabbitMQ binder in bulk, run the following command:

====
[source]
----
dataflow:>app import --uri http://bit.ly/Darwin-SR1-stream-applications-rabbit-maven
----
====

For more details, review how to xref:spring-cloud-dataflow-register-apps[register applications].

Data Flow delegates the Streams deployment to Skipper which provide support for features such as Streams update and rollback.

=== Creating Streams

NOTE: Make sure the Skipper server is deployed and have configured the Data Flow server's `SPRING_CLOUD_SKIPPER_CLIENT_SERVER_URI` property to reference the Skipper server.

The following example shows how to create and deploy a stream:

====
[source]
----
dataflow:> stream create --name httptest --definition "http | log"
dataflow:> stream deploy --name httptest --platformName pws
----
====

NOTE: You need to wait a little while until the applications are actually deployed before posting data. Tail the log file for each application to verify that the application has started.

Now you can post some data. The URL is unique to your deployment. The following example shows how to post data:

====
[source]
----
dataflow:> http post --target http://dataflow-AxwwAhK-httptest-http.cfapps.io --data "hello world"
----
====

Now you can see whether `hello world` is in the log files for the `log` application.

NOTE: Skipper includes the concept of link:https://docs.spring.io/spring-cloud-skipper/docs/current/reference/htmlsingle/#platforms[platforms],
so it is important to define the "`accounts`" based on the project preferences.
In the preceding YAML file, the accounts map to `pws` as the platform. You can modify this, and you can have any number of platform definitions.
The https://docs.spring.io/spring-cloud-skipper/docs/current/reference/htmlsingle/[Spring Cloud Skipper reference guide] has more details.


You can read more about the general features of using Skipper to deploy streams in the <<spring-cloud-dataflow-stream-lifecycle>> section and how to upgrade a streams in the <<spring-cloud-dataflow-stream-lifecycle-update>> section.

[[streams-using-skipper]]
== Deploying Streams

This section proceeds with the assumption that Spring Cloud Data Flow, Spring Cloud Skipper, RDBMS, and your desired messaging middleware are all running in PWS.
The following listing shows the apps running in a sample org and space:

====
[source,console,options=nowrap]
----
$ cf apps                                                                                                           ✭
Getting apps in org ORG / space SPACE as email@pivotal.io...
OK

name                         requested state   instances   memory   disk   urls
skipper-server               started           1/1         1G       1G     skipper-server.cfapps.io
dataflow-server              started           1/1         1G       1G     dataflow-server.cfapps.io
----
====

The following example shows how to start the Data Flow shell for the Data Flow server:

====
[source,bash,subs=attributes]
----
$ java -jar spring-cloud-dataflow-shell-{dataflow-project-version}.jar
----
====

If the Data Flow Server and shell are not running on the same host, you can point the shell to the Data Flow server URL, as follows:

====
[source]
----
server-unknown:>dataflow config server http://dataflow-server.cfapps.io
Successfully targeted http://dataflow-server.cfapps.io
dataflow:>
----
====

Alternatively, you can pass in the `--dataflow.uri` command line option. The shell'sx `--help` command line option shows what options are available.

You can verify the available platforms in Skipper, as follows:

====
[source,console,options=nowrap]
----
dataflow:>stream platform-list
╔═══════╤════════════╤═════════════════════════════════════════════════════════════════════════════════════╗
║ Name  │    Type    │                                                 Description                         ║
╠═══════╪════════════╪═════════════════════════════════════════════════════════════════════════════════════╣
║pws    │cloudfoundry│org = [scdf-ci], space = [space-sabby], url = [https://api.run.pivotal.io]           ║
╚═══════╧════════════╧═════════════════════════════════════════════════════════════════════════════════════╝
----
====

We start by deploying a stream with the `time-source` pointing to `1.2.0.RELEASE` and `log-sink` pointing to `1.1.0.RELEASE`.
The goal is to perform a rolling upgrade of the `log-sink` application to `1.2.0.RELEASE`.

====
[source,console,options=nowrap]
----
dataflow:>app register --name time --type source --uri maven://org.springframework.cloud.stream.app:time-source-rabbit:1.2.0.RELEASE --force
Successfully registered application 'source:time'

dataflow:>app register --name log --type sink --uri maven://org.springframework.cloud.stream.app:log-sink-rabbit:1.1.0.RELEASE --force
Successfully registered application 'sink:log'

dataflow:>app info source:time
Information about source application 'time':
Resource URI: maven://org.springframework.cloud.stream.app:time-source-rabbit:1.2.0.RELEASE

dataflow:>app info sink:log
Information about sink application 'log':
Resource URI: maven://org.springframework.cloud.stream.app:log-sink-rabbit:1.1.0.RELEASE
----
====

When you create a stream, use a unique name (one that might not be taken by another application on PCF/PWS).

The following example shows how to create a deploy a stream

====
[source]
----
dataflow:>stream create ticker-314 --definition "time | log"
Created new stream 'ticker-314'
dataflow:>stream deploy ticker-314 --platformName pws
Deployment request has been sent for stream 'ticker-314'
----
====

NOTE: While deploying the stream, we supply `--platformName`, which indicates the platform repository (`pws`) to
use when deploying the stream applications with Skipper.

Now you can list the running applications again and see your applications in the list, as the following example shows:

====
[source,console,options=nowrap]
----
$ cf apps                                                                                                                                                                                                                                         [1h] ✭
Getting apps in org ORG / space SPACE as email@pivotal.io...

name                         requested state   instances   memory   disk   urls
ticker-314-log-v1            started           1/1         1G       1G     ticker-314-log-v1.cfapps.io
ticker-314-time-v1           started           1/1         1G       1G     ticker-314-time-v1.cfapps.io
skipper-server               started           1/1         1G       1G     skipper-server.cfapps.io
dataflow-server              started           1/1         1G       1G     dataflow-server.cfapps.io
----
====

Now you an verify the logs, as the following example shows:

====
[source,console,options=nowrap]
----
$ cf logs ticker-314-log-v1
...
...
2017-11-20T15:39:43.76-0800 [APP/PROC/WEB/0] OUT 2017-11-20 23:39:43.761  INFO 12 --- [ ticker-314.time.ticker-314-1] log-sink                                 : 11/20/17 23:39:43
2017-11-20T15:39:44.75-0800 [APP/PROC/WEB/0] OUT 2017-11-20 23:39:44.757  INFO 12 --- [ ticker-314.time.ticker-314-1] log-sink                                 : 11/20/17 23:39:44
2017-11-20T15:39:45.75-0800 [APP/PROC/WEB/0] OUT 2017-11-20 23:39:45.757  INFO 12 --- [ ticker-314.time.ticker-314-1] log-sink                                 : 11/20/17 23:39:45
----
====

Now you can verify the stream history, as the following example shows:

====
[source,console,options=nowrap]
----
dataflow:>stream history --name ticker-314
╔═══════╤════════════════════════════╤════════╤════════════╤═══════════════╤════════════════╗
║Version│        Last updated        │ Status │Package Name│Package Version│  Description   ║
╠═══════╪════════════════════════════╪════════╪════════════╪═══════════════╪════════════════╣
║1      │Mon Nov 20 15:34:37 PST 2017│DEPLOYED│ticker-314  │1.0.0          │Install complete║
╚═══════╧════════════════════════════╧════════╧════════════╧═══════════════╧════════════════╝
----
====

Now you can verify the package manifest in Skipper. The `log-sink` should be at `1.1.0.RELEASE`. The following example shows both the command to use and its output:

====
[source,yml,options=nowrap]
----
dataflow:>stream manifest --name ticker-314

---
# Source: log.yml
apiVersion: skipper.spring.io/v1
kind: SpringCloudDeployerApplication
metadata:
  name: log
spec:
  resource: maven://org.springframework.cloud.stream.app:log-sink-rabbit
  version: 1.1.0.RELEASE
  applicationProperties:
    spring.cloud.dataflow.stream.app.label: log
    spring.cloud.stream.metrics.properties: spring.application.name,spring.application.index,spring.cloud.application.*,spring.cloud.dataflow.*
    spring.cloud.stream.bindings.applicationMetrics.destination: metrics
    spring.cloud.dataflow.stream.name: ticker-314
    spring.metrics.export.triggers.application.includes: integration**
    spring.cloud.stream.metrics.key: ticker-314.log.${spring.cloud.application.guid}
    spring.cloud.stream.bindings.input.group: ticker-314
    spring.cloud.dataflow.stream.app.type: sink
    spring.cloud.stream.bindings.input.destination: ticker-314.time
  deploymentProperties:
    spring.cloud.deployer.indexed: true
    spring.cloud.deployer.group: ticker-314

---
# Source: time.yml
apiVersion: skipper.spring.io/v1
kind: SpringCloudDeployerApplication
metadata:
  name: time
spec:
  resource: maven://org.springframework.cloud.stream.app:time-source-rabbit
  version: 1.2.0.RELEASE
  applicationProperties:
    spring.cloud.dataflow.stream.app.label: time
    spring.cloud.stream.metrics.properties: spring.application.name,spring.application.index,spring.cloud.application.*,spring.cloud.dataflow.*
    spring.cloud.stream.bindings.applicationMetrics.destination: metrics
    spring.cloud.dataflow.stream.name: ticker-314
    spring.metrics.export.triggers.application.includes: integration**
    spring.cloud.stream.metrics.key: ticker-314.time.${spring.cloud.application.guid}
    spring.cloud.stream.bindings.output.producer.requiredGroups: ticker-314
    spring.cloud.stream.bindings.output.destination: ticker-314.time
    spring.cloud.dataflow.stream.app.type: source
  deploymentProperties:
    spring.cloud.deployer.group: ticker-314
----
====

Now you can update `log-sink` from `1.1.0.RELEASE` to `1.2.0.RELEASE`.  First we need to register the version 1.2.0.RELEASE. The following example shows how to do so:

====
[source,console,options=nowrap]
----
dataflow:>app register --name log --type sink --uri maven://org.springframework.cloud.stream.app:log-sink-rabbit:1.1.0.RELEASE --force
Successfully registered application 'sink:log'
----
====

If you run the `app list` command for the log sink, you can now see that two versions are registered, as the following example shows:

====
[source,console,options=nowrap]
----
dataflow:>app list --id sink:log
╔══════╤═════════╤═════════════════════╤════╗
║source│processor│        sink         │task║
╠══════╪═════════╪═════════════════════╪════╣
║      │         │> log-1.1.0.RELEASE <│    ║
║      │         │log-1.2.0.RELEASE    │    ║
╚══════╧═════════╧═════════════════════╧════╝
----
====

The greater-than and less-than signs around `> log-1.1.0.RELEASE <` indicate that this is the default version that is used when matching `log` in the DSL for a stream definition.
You can change the default version by using the `app default` command.

====
[source,console,options=nowrap]
----
dataflow:>stream update --name ticker-314 --properties version.log=1.2.0.RELEASE
Update request has been sent for stream 'ticker-314'
----
====

Now you can list the applications again to see the two versions of the `ticker-314-log` application, as the following example shows:

====
[source,console,options=nowrap]
----
± cf apps                                                                                                                                                                                                                                         [1h] ✭
Getting apps in org ORG / space SPACE as email@pivotal.io...

Getting apps in org scdf-ci / space space-sabby as sanandan@pivotal.io...
OK

name                         requested state   instances   memory   disk   urls
ticker-314-log-v2            started           1/1         1G       1G     ticker-314-log-v2.cfapps.io
ticker-314-log-v1            stopped           0/1         1G       1G
ticker-314-time-v1           started           1/1         1G       1G     ticker-314-time-v1.cfapps.io
skipper-server               started           1/1         1G       1G     skipper-server.cfapps.io
dataflow-server              started           1/1         1G       1G     dataflow-server.cfapps.io
----
====

NOTE: There are two versions of the `log-sink` applications. The `ticker-314-log-v1` application instance is going down (route already removed) and the newly spawned `ticker-314-log-v2` application is bootstrapping.
The version number is incremented and the version-number (`v2`) is included in the new application name.

. Once the new application is up and running, you can verify the logs, as the following example shows:

====
[source,console,options=nowrap]
----
$ cf logs ticker-314-log-v2
...
...
2017-11-20T18:38:35.00-0800 [APP/PROC/WEB/0] OUT 2017-11-21 02:38:35.003  INFO 18 --- [ticker-314.time.ticker-314-1] ticker-314-log-v2                              : 11/21/17 02:38:34
2017-11-20T18:38:36.00-0800 [APP/PROC/WEB/0] OUT 2017-11-21 02:38:36.004  INFO 18 --- [ticker-314.time.ticker-314-1] ticker-314-log-v2                              : 11/21/17 02:38:35
2017-11-20T18:38:37.00-0800 [APP/PROC/WEB/0] OUT 2017-11-21 02:38:37.005  INFO 18 --- [ticker-314.time.ticker-314-1] ticker-314-log-v2                              : 11/21/17 02:38:36
----
====

Now you can look at the updated package manifest persisted in Skipper.
You should now be seeing `log-sink` at 1.2.0.RELEASE.
The following example shows the command to use and its output:

====
[source,yml,options=nowrap]
----
skipper:>stream manifest --name ticker-314
---
# Source: log.yml
apiVersion: skipper.spring.io/v1
kind: SpringCloudDeployerApplication
metadata:
  name: log
spec:
  resource: maven://org.springframework.cloud.stream.app:log-sink-rabbit
  version: 1.2.0.RELEASE
  applicationProperties:
    spring.cloud.dataflow.stream.app.label: log
    spring.cloud.stream.metrics.properties: spring.application.name,spring.application.index,spring.cloud.application.*,spring.cloud.dataflow.*
    spring.cloud.stream.bindings.applicationMetrics.destination: metrics
    spring.cloud.dataflow.stream.name: ticker-314
    spring.metrics.export.triggers.application.includes: integration**
    spring.cloud.stream.metrics.key: ticker-314.log.${spring.cloud.application.guid}
    spring.cloud.stream.bindings.input.group: ticker-314
    spring.cloud.dataflow.stream.app.type: sink
    spring.cloud.stream.bindings.input.destination: ticker-314.time
  deploymentProperties:
    spring.cloud.deployer.indexed: true
    spring.cloud.deployer.group: ticker-314
    spring.cloud.deployer.count: 1

---
# Source: time.yml
apiVersion: skipper.spring.io/v1
kind: SpringCloudDeployerApplication
metadata:
  name: time
spec:
  resource: maven://org.springframework.cloud.stream.app:time-source-rabbit
  version: 1.2.0.RELEASE
  applicationProperties:
    spring.cloud.dataflow.stream.app.label: time
    spring.cloud.stream.metrics.properties: spring.application.name,spring.application.index,spring.cloud.application.*,spring.cloud.dataflow.*
    spring.cloud.stream.bindings.applicationMetrics.destination: metrics
    spring.cloud.dataflow.stream.name: ticker-314
    spring.metrics.export.triggers.application.includes: integration**
    spring.cloud.stream.metrics.key: ticker-314.time.${spring.cloud.application.guid}
    spring.cloud.stream.bindings.output.producer.requiredGroups: ticker-314
    spring.cloud.stream.bindings.output.destination: ticker-314.time
    spring.cloud.dataflow.stream.app.type: source
  deploymentProperties:
    spring.cloud.deployer.group: ticker-314
----
====

Now you can verify stream history for the latest updates.

====
[source,console,options=nowrap]
----
dataflow:>stream history --name ticker-314
╔═══════╤════════════════════════════╤════════╤════════════╤═══════════════╤════════════════╗
║Version│        Last updated        │ Status │Package Name│Package Version│  Description   ║
╠═══════╪════════════════════════════╪════════╪════════════╪═══════════════╪════════════════╣
║2      │Mon Nov 20 15:39:37 PST 2017│DEPLOYED│ticker-314  │1.0.0          │Upgrade complete║
║1      │Mon Nov 20 15:34:37 PST 2017│DELETED │ticker-314  │1.0.0          │Delete complete ║
╚═══════╧════════════════════════════╧════════╧════════════╧═══════════════╧════════════════╝
----
====

Rolling-back to the previous version is just a command away.
The following example shows how to do so and the resulting output:

====
[source,console,options=nowrap]
----
dataflow:>stream rollback --name ticker-314
Rollback request has been sent for the stream 'ticker-314'

...
...

dataflow:>stream history --name ticker-314
╔═══════╤════════════════════════════╤════════╤════════════╤═══════════════╤════════════════╗
║Version│        Last updated        │ Status │Package Name│Package Version│  Description   ║
╠═══════╪════════════════════════════╪════════╪════════════╪═══════════════╪════════════════╣
║3      │Mon Nov 20 15:41:37 PST 2017│DEPLOYED│ticker-314  │1.0.0          │Upgrade complete║
║2      │Mon Nov 20 15:39:37 PST 2017│DELETED │ticker-314  │1.0.0          │Delete complete ║
║1      │Mon Nov 20 15:34:37 PST 2017│DELETED │ticker-314  │1.0.0          │Delete complete ║
╚═══════╧════════════════════════════╧════════╧════════════╧═══════════════╧════════════════╝
----
====


[[getting-started-deploying-tasks]]
== Deploying Tasks

To run a simple task application, you can register all the out-of-the-box task applications with the following command:

====
[source]
----
dataflow:>app import --uri http://bit.ly/Dearborn-GA-task-applications-maven
----
====

Now you can create a simple link:https://docs.spring.io/spring-cloud-task-app-starters/docs/Dearborn.RELEASE/reference/htmlsingle/#spring-cloud-task-modules-tasks[timestamp] task, as the following example shows:

====
[source]
----
dataflow:>task create mytask --definition "timestamp --format='yyyy'"
----
====

Now you can examine the tail of the logs (for example, `cf logs mytask`) and then launch the task in the UI or in the Data Flow Shell, as the following example shows:

====
[source]
----
dataflow:>task launch mytask
----
====

You will see the year (`2018` at the time of this writing) printed in the logs. The execution status of the task is stored in the database, and you can retrieve information about the task execution by using the `task execution list` and `task execution status --id <ID_OF_TASK>` shell commands or though the Data Flow UI.

NOTE: The current underlying PCF task capabilities are considered experimental for PCF version versions less than 1.9.
See http://docs.spring.io/spring-cloud-dataflow/docs/{scdf-core-version}/reference/htmlsingle/#enable-disable-specific-features[Feature Togglers] for how to disable task support in Data Flow.
