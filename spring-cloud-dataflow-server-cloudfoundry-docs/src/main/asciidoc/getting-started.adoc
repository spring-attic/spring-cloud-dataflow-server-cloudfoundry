[[getting-started]]
= Getting started

== Deploying on Cloud Foundry

Spring Cloud Data Flow can be used to deploy modules in a Cloud Foundry environment. When doing so, the
server application can either run itself on Cloud Foundry, or on another installation (e.g. a simple laptop).

The required configuration amounts to the same in either case, and is merely related to providing credentials to the
Cloud Foundry instance so that the server can spawn applications itself. Any Spring Boot compatible configuration
mechanism can be used (passing program arguments, editing configuration files before building the application, using
link:https://github.com/spring-cloud/spring-cloud-config[Spring Cloud Config], using environment variables, etc.),
although some may prove more practicable than others when running _on_ Cloud Foundry.

NOTE: By default, the https://github.com/spring-cloud/spring-cloud-dataflow/tree/master/spring-cloud-dataflow-registry[application registry] in Spring Cloud Data Flow's Cloud Foundry server is empty. It is intentionally designed to allow users to have the flexibility of http://docs.spring.io/spring-cloud-dataflow/docs/{scdf-core-version}/reference/html/_dsl_syntax.html#_register_a_stream_app[choosing and registering] applications, as they find appropriate for the given use-case requirement. Depending on the message-binder of choice, users can register between http://repo.spring.io/libs-snapshot/org/springframework/cloud/stream/app/[RabbitMQ or Apache Kafka] based maven artifacts.

=== Provision a Redis service instance on Cloud Foundry.
Use `cf marketplace` to discover which plans are available to you, depending on the details of your Cloud Foundry setup.
For example when using link:https://run.pivotal.io/[Pivotal Web Services]:

```
cf create-service rediscloud 30mb redis
```

=== Provision a Rabbit service instance on Cloud Foundry.
Use `cf marketplace` to discover which plans are available to you, depending on the details of your Cloud Foundry setup.
For example when using link:https://run.pivotal.io/[Pivotal Web Services]:

```
cf create-service cloudamqp lemur rabbit
```

=== Provision a MySQL service instance on Cloud Foundry.
Use `cf marketplace` to discover which plans are available to you, depending on the details of your Cloud Foundry setup.
For example when using link:https://run.pivotal.io/[Pivotal Web Services]:

```
cf create-service p_mysql 100mb my_mysql
```

=== Download the Spring Cloud Data Flow Server and Shell apps:

[subs=attributes]
```
wget http://repo.spring.io/{version-type-lowercase}/org/springframework/cloud/spring-cloud-dataflow-server-cloudfoundry/{project-version}/spring-cloud-dataflow-server-cloudfoundry-{project-version}.jar
wget http://repo.spring.io/{dataflow-version-type-lowercase}/org/springframework/cloud/spring-cloud-dataflow-shell/{dataflow-project-version}/spring-cloud-dataflow-shell-{dataflow-project-version}.jar
```

You can either deploy the server application on Cloud Foundry itself or on your local machine.
The following two sections explain each way of running the server.

[[running-on-cloudfoundry]]
=== Deploying the Server app on Cloud Foundry

Push the server application on Cloud Foundry, configure it (see below) and start it.

NOTE: You must use a unique name for your app; an app with the same name in the same organization will cause your
deployment to fail

[subs=attributes]
```
cf push dataflow-server --no-start -p spring-cloud-dataflow-server-cloudfoundry-{project-version}.jar
cf bind-service dataflow-server redis
cf bind-service dataflow-server rabbit
```

NOTE: If you are pushing to a space with multiple users, for example on PWS, there may already be a route taken for the
applicaiton name you have chosen. You can use the options `--random-route` to avoid this when pushing the app.

Now we can configure the app. The following configuration is for Pivotal Web Services. You need to fill in \{org}, \{space},
\{email} and \{password} before running these commands.

NOTE: Only set 'Skip SSL Validation' to true if you're running on a Cloud Foundry instance using self-signed
certs (e.g. in development). Do not use for production.

NOTE: If you are deploying in an environment that requires you to sign on using the Pivotal Single Sign-On Service, refer to the section <<getting-started-security-cloud-foundry>> for information on how to configure the server.

```
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_URL https://api.run.pivotal.io
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_ORG {org}
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SPACE {space}
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_DOMAIN cfapps.io
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SERVICES redis,rabbit
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_USERNAME {email}
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_PASSWORD {password}
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SKIP_SSL_VALIDATION false
```

Spring Cloud Data Flow server implementations (cf, mesos, yarn, or kubernetes) do not have
'any' default remote maven repository configured. This is intentionally designed to provide the flexibility for
the users, so they can override and point to a remote repository of their choice. The out-of-the-box
applications that are supported by Spring Cloud Data Flow are available in Spring's repository,
so if you want to use them, you 'must' set it as the remote repository as listed below.

```
cf set-env dataflow-server MAVEN_REMOTE_REPOSITORIES_REPO1_URL https://repo.spring.io/libs-snapshot
```
where `repo1` is the alias name for the remote repository.

You can also set other optional properties for deployment to Cloud Foundry.

* You can set the buildpack that will be used to deploy the application.  For example, to use the Java offline buildback, set the following environment variable

```
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_BUILDPACK java_buildpack_offline
```

* If you'd like to use `config-server` to manage centralized configurations for all the applications orchestrated by
Spring Cloud Data Flow, you can set it up like the following.

```
cf set-env dataflow-server SPRING_APPLICATION_JSON '{"spring.cloud.dataflow.applicationProperties.stream.spring.cloud.config.uri": "http://<CONFIG_SERVER_URI>"}'
```

* The default memory and disk sizes for a deployed application can also be configured. By default they are 1024 MB memory
and 1024 MB disk.  Thse are controlled by setting an integer value, representing the number of MB, to the following
properties, `spring.cloud.deployer.cloudfoundry.memory` and `spring.cloud.deployer.cloudfoundry.disk`.
The default number of instances to deploy is set to 1, but can be overridden using with the
`spring.cloud.deployer.cloudfoundry.instances` property.  All these properties are `@ConfigurationProperties` of the
Cloud Foundry deployer. See link:https://github.com/spring-cloud/spring-cloud-deployer-cloudfoundry/blob/master/src/main/java/org/springframework/cloud/deployer/spi/cloudfoundry/CloudFoundryDeployerProperties.java[CloudFoundryDeployerProperties.java] for more information.

We are now ready to start the app.

```
cf start dataflow-server
```

Alternatively, you can run the Admin application locally on your machine which is described in the next section.

=== Running the Server app locally

To run the server application locally, targeting your Cloud Foundry installation, you you need to configure the
application either by passing in command line arguments (see below) or setting a number of environment variables.

To use environment variables set the following:

```
export SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_URL=https://api.run.pivotal.io
export SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_ORG={org}
export SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SPACE={space}
export SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_DOMAIN=cfapps.io
export SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SERVICES=redis,rabbit
export SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_USERNAME={email}
export SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_PASSWORD={password}
export SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SKIP_SSL_VALIDATION=false
```

You need to fill in \{org}, \{space}, \{email} and \{password} before running these commands.

NOTE: Only set 'Skip SSL Validation' to true if you're running on a Cloud Foundry instance using self-signed
certs (e.g. in development). Do not use for production.

Now we are ready to start the server application:

[subs=attributes]
```
java -jar spring-cloud-dataflow-server-cloudfoundry-{project-version}.jar [--option1=value1] [--option2=value2] [etc.]
```

=== Running Spring Cloud Data Flow Shell locally

Run the shell and optionally target the Admin application if not running on the same host (will typically be the case if
deployed on Cloud Foundry as explained xref:running-on-cloudfoundry[here])

[subs=attributes]
```
$ java -jar spring-cloud-dataflow-shell-{dataflow-project-version}.jar
```

```
server-unknown:>dataflow config server http://dataflow-server.cfapps.io
Successfully targeted http://dataflow-server.cfapps.io
dataflow:>
```

By default, the application registry will be empty. If you would like to register all out-of-the-box stream applications built with the RabbitMQ binder in bulk, you can with the following command. For more details, review how to link:http://docs.spring.io/spring-cloud-dataflow/docs/{scdf-core-version}/reference/html/spring-cloud-dataflow-register-apps.html[register applications].

```
dataflow:>app import --uri http://bit.ly/stream-applications-rabbit-maven

```

You can now use the shell commands to list available applications (source/processors/sink) and create streams. For example:

[source,bash]
----
dataflow:> stream create --name httptest --definition "http | log" --deploy
----

NOTE: You will need to wait a little while until the apps are actually deployed successfully
before posting data.  Tail the log file for each application to verify
the application has started.

Now post some data. The URL will be unique to your deployment, the following is just an example
[source,bash]
----
dataflow:> http post --target http://dataflow-nonconcentrative-knar-httptest-http.cfapps.io --data "hello world"
----
Look to see if `hello world` ended up in log files for the `log` application.

[[getting-started-security]]
== Security

By default, the Data Flow server is unsecured and runs on an unencrypted HTTP connection. You can secure your REST endpoints,
as well as the Data Flow Dashboard by enabling HTTPS and requiring clients to authenticate. More details about securing the
REST endpoints and configuring to authenticate against an OAUTH backend (_i.e: UAA/SSO running on Cloud Foundry_), please
review the security section from the core http://docs.spring.io/spring-cloud-dataflow/docs/{scdf-core-version}/reference/html/getting-started-security.html[reference guide]. The security configurations can be configured in `dataflow-server.yml` or passed as environment variables through `cf set-env` commands.

[[getting-started-app-names-cloud-foundry]]
== Application Names and Prefixes

To help avoid clashes with routes across spaces in Cloud Foundry, a naming strategy to provide a random prefix to a
deployed application is available and is enabled by default. The https://github.com/spring-cloud/spring-cloud-deployer-cloudfoundry#application-name-settings-and-deployments[default configurations]
are overridable and the respective properties can be set via `cf set-env` commands.

For instance, if you'd like to disable the randmoization, you can override it through:

```
cf set-env dataflow-server SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_ENABLE_RANDOM_APP_NAME_PREFIX false
```

[[getting-started-security-cloud-foundry]]
== Authentication and Cloud Foundry

When deploying Spring Cloud Data Flow to Cloud Foundry, you can take advantage of the
 https://github.com/pivotal-cf/spring-cloud-sso-connector[_Spring Cloud Single Sign-On Connector_],
 which provides Cloud Foundry specific auto-configuration support for OAuth 2.0,
 when used in conjunction with the _Pivotal Single Sign-On Service_.

Simply set `security.basic.enabled` to `true` and in Cloud Foundry bind the SSO
service to your Data Flow Server app and SSO will be enabled.

== Configuration Reference

The following pieces of configuration must be provided.  These are Spring Boot @ConfigurationProperties so you can set
them as environment variables or by any other means that Spring Boot supports.  Here is a listing in environment
variable format as that is an easy way to get started configuring Boot applications in Cloud Foundry.

```
# Default values cited after the equal sign.
# Example values, typical for Pivotal Web Services, cited as a comment

# url of the CF API (used when using cf login -a for example), e.g. https://api.run.pivotal.io
# (for setting env var use SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_URL)
spring.cloud.deployer.cloudfoundry.url=

# name of the organization that owns the space above, e.g. youruser-org
# (For Setting Env var use SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_ORG)
spring.cloud.deployer.cloudfoundry.org=

# name of the space into which modules will be deployed
# (for setting env var use SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SPACE)
spring.cloud.deployer.cloudfoundry.space=<same space as server when running on CF, or 'development'>

# the root domain to use when mapping routes, e.g. cfapps.io
# (for setting env var use SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_DOMAIN)
spring.cloud.deployer.cloudfoundry.domain=

# Comma separated set of service instance names to bind to the module.
# Amongst other things, this should include a service that will be used
# for Spring Cloud Stream binding
# (for setting env var use SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SERVICES)
spring.cloud.deployer.cloudfoundry.services=redis,rabbit

# username and password of the user to use to create apps (modules)
# (for setting env var use SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_USERNAME and SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_PASSWORD)
spring.cloud.deployer.cloudfoundry.username=
spring.cloud.deployer.cloudfoundry.password=

# Whether to allow self-signed certificates during SSL validation
# (for setting env var use SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_SKIP_SSL_VALIDATION)
spring.cloud.deployer.cloudfoundry.skipSslValidation=false
```

Note that you can set the following properties `spring.cloud.deployer.cloudfoundry.services`,
`spring.cloud.deployer.cloudfoundry.memory`, and `spring.cloud.deployer.cloudfoundry.disk` as part of an individual deployment request prefixed by the `app.<name of application>`.  For example

```
>stream create --name ticktock --definition "time | log"
>stream deploy --name ticktock --properties "app.time.spring.cloud.deployer.cloudfoundry.memory=2048"
```

will deploy the time source with 2048MB of memory, while the log sink will use the default 1024MB.

[[getting-started-service-binding-at-application-level]]
== Application Level Service Bindings
When deploying streams in Cloud Foundry, you can take advantage of application specific service bindings, so not all
services are globally configured for all the apps orchestrated by Spring Cloud Data Flow.

For instance, if you'd like to provide `mysql` service binding only for the `jdbc` application in the following stream definition, you can pass the service binding as a deployment property.

[source,bash]
----
dataflow:>stream create --name httptojdbc --definition "http | jdbc"
dataflow:> stream deploy --name httptojdbc --properties "app.jdbc.spring.cloud.deployer.cloudfoundry.services=mysqlService"
----

Where, `mysqlService` is the name of the service specifically only bound to `jdbc` application and the `http` application wouldn't get the binding by this method. If you have more than one service to bind, they can be passed as comma separated items (_eg: app.jdbc.spring.cloud.deployer.cloudfoundry.services=mysqlService,someService_).

[[getting-started-service-application-rolling-upgrades]]
== Application Rolling Upgrades
Similar to Cloud Foundry's https://docs.pivotal.io/pivotalcf/1-7/devguide/deploy-apps/blue-green.html[blue-green] deployments,
you can perform rolling upgrades on the applications orchestrated by Spring Cloud Data Flow.

Let's start with the following simple stream definition.

[source,bash]
----
dataflow:>stream create --name foo --definition "time | log" --deploy
----

List Apps.

[source,bash]
----
→ cf apps
Getting apps in org test-org / space development as test@pivotal.io...
OK

name       requested state   instances   memory   disk   urls
foo-log    started           1/1         1G       1G     foo-log.cfapps.io
foo-time   started           1/1         1G       1G     foo-time.cfapps.io
----

Let's assume you've to make an enhancement to update the "logger" to append extra text in every log statement.

* Download the `Log Sink` application starter with "Rabbit binder starter" from http://start-scs.cfapps.io/
* Load the downloaded project in an IDE
* Import the `LogSinkConfiguration.class`
* Adapt the handler to add extra text: `loggingHandler.setLoggerName("TEST [" + this.properties.getName() + "]");`
* Build the application locally

[source,java]
----
@SpringBootApplication
@Import(LogSinkConfiguration.class)
public class DemoApplication {

	@Autowired
	private LogSinkProperties properties;

	public static void main(String[] args) {
		SpringApplication.run(DemoApplication.class, args);
	}

	@Bean
	@ServiceActivator(inputChannel = Sink.INPUT)
	public LoggingHandler logSinkHandler() {
		LoggingHandler loggingHandler = new LoggingHandler(this.properties.getLevel().name());
		loggingHandler.setExpression(this.properties.getExpression());
		loggingHandler.setLoggerName("TEST [" + this.properties.getName() + "]");
		return loggingHandler;
	}
}
----

Let's deploy the locally built application to Cloud Foundry

[source,bash]
----
→ cf push foo-log-v2 -p demo-0.0.1-SNAPSHOT.jar -n foo-log-v2 --no-start
----

List Apps.

[source,bash]
----
→ cf apps
Getting apps in org test-org / space development as test@pivotal.io...
OK

name       requested state   instances   memory   disk   urls
foo-log    started           1/1         1G       1G     foo-log.cfapps.io
foo-time   started           1/1         1G       1G     foo-time.cfapps.io
foo-log-v2 stopped           1/1         1G       1G     foo-log-v2.cfapps.io
----

The stream applications do not communicate via (Go)Router, so they aren't generating HTTP traffic. Instead, they
communicate via the underlying messaging middleware such as Kafka or RabbitMQ. In order to rolling upgrade to route the
payload from old to the new version of the application, you'd have to replicate the `SPRING_APPLICATION_JSON` environment
variable from the old application that includes `spring.cloud.stream.bindings.input.destination` and `spring.cloud.stream.bindings.input.group` credentials.

NOTE: You can find the `SPRING_APPLICATION_JSON` of the old application via: `"cf env foo-log"`.

[source,bash]
----
cf set-env foo-log-v2 SPRING_APPLICATION_JSON '{"spring.cloud.stream.bindings.input.destination":"foo.time","spring.cloud.stream.bindings.input.group":"foo"}'
----

Let's start `foo-log-v2` application.

[source,bash]
----
cf start foo-log-v2
----

As soon as the application bootstraps, you'd now notice the payload being load balanced between two log application
instances running on Cloud Foundry. Since they both share the same "destination" and "consumer group", they are now
acting as competing consumers.

Old App Logs:

[source,bash]
----
2016-08-08T17:11:08.94-0700 [APP/0]      OUT 2016-08-09 00:11:08.942  INFO 19 --- [ foo.time.foo-1] log.sink                                 : 08/09/16 00:11:08
2016-08-08T17:11:10.95-0700 [APP/0]      OUT 2016-08-09 00:11:10.954  INFO 19 --- [ foo.time.foo-1] log.sink                                 : 08/09/16 00:11:10
2016-08-08T17:11:12.94-0700 [APP/0]      OUT 2016-08-09 00:11:12.944  INFO 19 --- [ foo.time.foo-1] log.sink                                 : 08/09/16 00:11:12
----

New App Logs:

[source,bash]
----
2016-08-08T17:11:07.94-0700 [APP/0]      OUT 2016-08-09 00:11:07.945  INFO 26 --- [ foo.time.foo-1] TEST [log.sink                       : 08/09/16 00:11:07]
2016-08-08T17:11:09.92-0700 [APP/0]      OUT 2016-08-09 00:11:09.925  INFO 26 --- [ foo.time.foo-1] TEST [log.sink                       : 08/09/16 00:11:09]
2016-08-08T17:11:11.94-0700 [APP/0]      OUT 2016-08-09 00:11:11.941  INFO 26 --- [ foo.time.foo-1] TEST [log.sink                       : 08/09/16 00:11:11]
----

Deleting the old version `foo-log` from the CF CLI would make all the payload consumed by the `foo-log-v2` application. Now,
you've successfully upgraded an application in the streaming pipeline without bringing it down in entirety to do
an adjustment in it.

List Apps.

[source,bash]
----
→ cf apps
Getting apps in org test-org / space development as test@pivotal.io...
OK

name       requested state   instances   memory   disk   urls
foo-time   started           1/1         1G       1G     foo-time.cfapps.io
foo-log-v2 started           1/1         1G       1G     foo-log-v2.cfapps.io
----

NOTE: A comprehensive canary analysis along with rolling upgrades will be supported via http://www.spinnaker.io/[Spinnaker]
in future releases.