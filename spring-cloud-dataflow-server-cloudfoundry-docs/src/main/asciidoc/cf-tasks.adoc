[[tasks-on-cloudfoundry]]
= Tasks on Cloud Foundry

Spring Cloud Data Flow's task functionality exposes new task capabilities within
the Pivotal Cloud Foundry runtime. It is important to note that the current underlying PCF
task capabilities are considered experimental for PCF version versions less than 1.9.  See
 <<enable-disable-specific-features>> for how to disable task support in Data Flow.

== Version Compatibility

The task functionality depends on the latest versions of PCF for runtime support. This
release requires PCF version 1.7.12 or higher to run tasks.  Tasks are an experimental
feature in PCF 1.7 and 1.8 and a GA feature in PCF 1.9.

== Task Database Schema

The database schema for Task applications was changed slighlty from the 1.0.x to 1.1.x version of
Spring Cloud Task.  Since Spring Cloud Data Flow automatically creates the database schema if it is
not present upon server startup, you may need to update the schema if you ran a 1.0.x version of the
Data Flow server and now are upgrading to the 1.1.x version.  You can find the migration scripts
link:https://github.com/spring-cloud/spring-cloud-task/tree/1.1.0.RELEASE/spring-cloud-task-core/src/main/resources/org/springframework/cloud/task/migration[here]
in the Spring Cloud Task Github repository.  The documentation for
link:https://docs.cloudfoundry.org/devguide/deploy-apps/ssh-services.html[Accessing Services with Diego SSH]
and this link:https://pivotaljourney.blogspot.com/2016/05/connecting-gui-tool-to-mysql-service-in.html[blog entry]
for connecting a GUI tools to the MySQL Service in PCF should help you to update the schema.

== Running Task Applications

Running a task application within Spring Cloud Data Flow goes through a slightly different
lifecycle than running a stream application. Both types of applications need to be registered
with the appropriate artifact coordinates. Both need a definition created via the SCDF DSL.
However, that's where the similarities end.

With stream based applications, you "deploy" them with the intent that they run until they
are undeployed. A stream definition is only deployed once (it can be scaled, but only
deployed as one instance of the stream as a whole). However, tasks are _launched_. A single
task definition can be launched many times. With each launch, they will start, execute,
and shut down with PCF cleaning up the resources once the shutdown has occurred. The
following sections outline the process of creating, launching, destroying, and viewing tasks.

=== Create a Task

Similar to streams, creating a task application is done via the SCDF DSL or through the
dashboard. To create a task definition in SCDF, you've to either develop a task
application or use one of the out-of-the-box link:https://docs.spring.io/spring-cloud-task-app-starters/docs/{sct-starters-core-version}/reference/htmlsingle[task app-starters].
The maven coordinates of the task application should be registered in SCDF. For more
details on how to register task applications, review <<_registering_a_task_application,register task applications>>
section from the core docs.

Let's see an example that uses the out-of-the-box `timestamp` task application.

[source]
----
dataflow:>task create --name foo --definition "timestamp"
Created new task 'foo'
----

NOTE: Tasks in SCDF do not require explicit deployment. They are required to be launched
and with that there are different ways to launch them - refer to <<spring-cloud-dataflow-launch-tasks-from-stream,this section>>
for more details.

=== Launch a Task

Unlike streams, tasks in SCDF requires an explicit launch trigger or it can be manually kicked-off.

[source]
----
dataflow:>task launch foo
Launched task 'foo'
----

=== Launch a Task with arguments and properties

When a task is launched, any properties that need to be passed as command line arguments to the task application can be set when launching the task, as follows:

```
dataflow:>task launch mytask --arguments "--key1=value1,--key2=value2"

```

Additional properties meant for a `TaskLauncher` itself can be passed in by using a `--properties` option.
The format of this option is a comma-separated string of properties prefixed with `app.<task definition name>.<property>`.
Properties are passed to `TaskLauncher` as application properties.
It is up to an implementation to choose how those are passed into an actual task application.
If the property is prefixed with `deployer` instead of `app`, it is passed to `TaskLauncher` as a deployment property and its meaning may be `TaskLauncher` implementation specific.

```
dataflow:>task launch mytask --properties "deployer.timestamp.custom1=value1,app.timestamp.custom2=value2"

```

One can also pass JAVA_OPTS as the CF deployer property when the task is launched.

```
task launch --name mytask --properties "deployer.mytask.cloudfoundry.javaOpts=-Duser.timezone=America/New_York"
```

The JAVA_OPTS can also be set as the global property for all the tasks using
`SPRING_CLOUD_DEPLOYER_CLOUDFOUNDRY_TASK_JAVA_OPTS`

=== View Task Logs

As previously mentioned, the CL CLI is the way to interact with tasks on PCF,
including viewing the logs. In order to view the logs as a task is executing use the
following command where `foo` is the name of the task you are executing:

[source,bash]
----
cf v3-logs foo
Tailing logs for app foo...

....
....
....
....

2016-08-19T09:44:49.11-0700 [APP/TASK/bar1/0]OUT 2016-08-19 16:44:49.111  INFO 7 --- [           main] o.s.c.t.a.t.TimestampTaskApplication     : Started TimestampTaskApplication in 2.734 seconds (JVM running for 3.288)
2016-08-19T09:44:49.13-0700 [APP/TASK/bar1/0]OUT Exit status 0
2016-08-19T09:44:49.19-0700 [APP/TASK/bar1/0]OUT Destroying container
2016-08-19T09:44:50.41-0700 [APP/TASK/bar1/0]OUT Successfully destroyed container
----

NOTE: Logs are only viewable through the CF CLI as the app is running. Historic
logs are not available.

=== List Tasks

Listing tasks is as simple as:

[source]
----
dataflow:>task list
╔══════════════════════╤═════════════════════════╤═══════════╗
║      Task Name       │     Task Definition     │Task Status║
╠══════════════════════╪═════════════════════════╪═══════════╣
║foo                   │timestamp                │complete   ║
╚══════════════════════╧═════════════════════════╧═══════════╝
----

=== List Task Executions

If you'd like to view the execution details of the launched task, you could do the following.

[source]
----
dataflow:>task execution list
╔════════════════════════╤══╤═════════════════════════╤═════════════════════════╤════════╗
║       Task Name        │ID│       Start Time        │        End Time         │  Exit  ║
║                        │  │                         │                         │  Code  ║
╠════════════════════════╪══╪═════════════════════════╪═════════════════════════╪════════╣
║foo:cloud:              │1 │ Fri Aug 19 09:44:49 PDT │Fri Aug 19 09:44:49 PDT  │0       ║
╚════════════════════════╧══╧═════════════════════════╧═════════════════════════╧════════╝
----

=== Destroy a Task

Destroying the task application from SCDF removes the task definition from task repository.

[source]
----
dataflow:>task destroy foo
Destroyed task 'foo'
dataflow:>task list
╔═════════╤═══════════════╤═══════════╗
║Task Name│Task Definition│Task Status║
╚═════════╧═══════════════╧═══════════╝
----

=== Deleting Task From Cloud Foundry
Currently Spring Cloud Data Flow does not delete tasks deployed on a Cloud
Foundry instance once they have been pushed.  The only way to do this now is via
CLI on a Cloud Foundry instance version 1.9 or above.
This is done in 2 steps:

. Obtain a list of the apps via the `cf apps` command.
. Identify the task app to be deleted and execute the `cf delete <task-name>`
command.

NOTE: The `task destroy <task-name>` only deletes the definition and not the task
deployed on Cloud Foundry.